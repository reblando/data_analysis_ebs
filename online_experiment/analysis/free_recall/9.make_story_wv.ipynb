{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Actual Story Vectors\n",
    "## 12/7/20\n",
    "## Make a dict, where for each story there is the wordvec made from the actual stories read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import copy\n",
    "from random import randrange\n",
    "from sklearn.metrics import jaccard_score\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA #for cluster analysis\n",
    "from gensim.models import KeyedVectors #for word embeddings\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "import os #for importing\n",
    "import pickle #for loading transcripts\n",
    "from scipy.stats import pearsonr \n",
    "\n",
    "# from _DRAFT_20200604_functions import * #includes constants and score function\n",
    "from tqdm import tqdm_notebook #for progress bar\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Imports\n",
    "## A. Read story texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "files1 = glob.glob('/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/*')\n",
    "len(files1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/32\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/34\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/33\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/11\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/42\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/21\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/44\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/43\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/31\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/41\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/24\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/23\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/12\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/13\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/14\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/fmri/story_texts_ns/22\n"
     ]
    }
   ],
   "source": [
    "for filenam in files1: \n",
    "    print(filenam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for filenam in files1: \n",
    "    with open(filenam, \"r\") as file: \n",
    "        if filenam in files: \n",
    "            continue \n",
    "        else:\n",
    "            x = filenam.split('ns/')[1]\n",
    "            files[x] = file.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Importing wiki vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipath = 'rolando/wiki-news-300d-1M.vec'\n",
    "wv_model = KeyedVectors.load_word2vec_format(wikipath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = 300\n",
    "\n",
    "# FastText preprocessing, based on bittlingmayer/ft_wiki_preproc.py\n",
    "# Remove special characters, put spaces between all tokens\n",
    "SUB = [\"s/’/'/g\", \"s/′/'/g\", \"s/''/ /g\", \"s/'/ ' /g\", 's/“/\"/g', 's/”/\"/g', 's/\"/ /g', \"s/\\\\./ \\\\. /g\", \"s/<br \\\\/>/ /g\", \"s/, / , /g\", \"s/(/ ( /g\", \"s/)/ ) /g\", \"s/\\\\!/ \\\\! /g\", \"s/\\\\?/ \\\\? /g\", \"s/\\\\;/ /g\", \"s/\\\\:/ /g\", \"s/-/ - /g\", \"s/=/ /g\", \"s/=/ /g\", \"s/*/ /g\", \"s/|/ /g\", \"s/«/ /g\", \n",
    "       \"s/…/ /g\", \"s/‘/ /g\", \"s/í/ /g\", \"s/ñ/ /g\", \"s/\\x84/ /g\", \"s/î/ /g\", \"s/ó/ /g\", \"s/\\x83/ /g\", \"s/ï/ /g\", \"s/õ/ /g\",\n",
    "       \"s/ò/ /g\", \"s/,/ /g\", \"s/ô/ /g\", \"s/\\x92/ /g\", \"s/é/ /g\", \"s/\\x8e/ /g\", \"s/â\\x80¦/ /g\", \"s/\\x91/ /g\", \"s/\\x93/ /g\",\n",
    "       \"s/\\x94/ /g\", \"s/ã®/ /g\", \"s/ã¨/ /g\", \"s/ã©/ /g\",\n",
    "       \"s/\\â\\x80\\x99/ /g\", \"s/â\\x80\\x9c/ /g\", \"s/â\\x80\\x9d/ /g\", \"s/â\\x80\\x99/ /g\", \"s/â\\x80\\x9c/ /g\", \"s/â\\x80\\x98/ /g\",\n",
    "       \"s/â/ /g\"]\n",
    "\n",
    "def __normalize_text(s):\n",
    "    for sg in SUB:\n",
    "        rep = sg.replace('\\\\','').split('/')\n",
    "        s = s.replace(rep[1], rep[2])\n",
    "    s = s.replace('/',' ')\n",
    "    return s\n",
    "\n",
    "def __spaces(s):\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def __digits(s):\n",
    "    return ''.join(filter(lambda c: not c.isdigit(), s))\n",
    "\n",
    "# def preproc(s):\n",
    "#     return __punctuation(__spaces(__digits(__normalize_text(s.lower()))))\n",
    "\n",
    "def preproc(s):\n",
    "    return (__spaces(__digits(__normalize_text(s.lower()))))\n",
    "\n",
    "def __punctuation(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def word2vecSent(sentence, model = 'fasttext'):\n",
    "    wv_dim = 300 #for glove and fasttext\n",
    "    \n",
    "    if model == 'glove':\n",
    "        wvmodel = glove_model\n",
    "    elif model == 'fasttext':\n",
    "        wvmodel = wv_model\n",
    "        \n",
    "    words = preproc(sentence).split(' ')\n",
    "    wv = np.zeros((len(words), wv_dim))\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in wvmodel.vocab:\n",
    "            wv[i,:] = wvmodel.word_vec(words[i])\n",
    "    \n",
    "    return wv, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Making the actual story vectors\n",
    "### A. Getting Word Vectors for each free recall \n",
    "#### dict{story: story_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_matrices = {}\n",
    "gran_stories = {}\n",
    "\n",
    "# Iterating through each key\n",
    "for file in files:\n",
    "    these_wvs, these_words = word2vecSent(files[file])\n",
    "    # Story template\n",
    "    wv_matrices[file] = np.mean(these_wvs, axis = 0)\n",
    "    # Matrix with vectors of individual words, and list with words\n",
    "    gran_stories[file] = {'wvs': these_wvs, 'words': these_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Export Word Vector Matrices and sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( wv_matrices, open( 'actual_story_vectors', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Export gran_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( gran_stories, open( 'gran_stories', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
