{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Top Ten Words for Story WV, Loc Template WV, and Soc Template WV\n",
    "## For each story there will be 3 'Top Ten Words', One for each type of regressor\n",
    "## For each regressor, regress out of the recall wv the other two vectors. Do this for each participant.\n",
    "## Correlate the residual wvs with each word from the target regressor. \n",
    "## Sum correlation values across participants\n",
    "## Index the 10 highest values, and find the 10 highest words\n",
    "## Put in dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import copy\n",
    "from random import randrange\n",
    "from sklearn.metrics import jaccard_score\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA #for cluster analysis\n",
    "from gensim.models import KeyedVectors #for word embeddings\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "import os #for importing\n",
    "import pickle #for loading transcripts\n",
    "from scipy.stats import pearsonr \n",
    "\n",
    "# from _DRAFT_20200604_functions import * #includes constants and score function\n",
    "from tqdm import tqdm_notebook #for progress bar\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing story and template vectors and sums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = pickle.load( open( 'c_template', \"rb\" ) )\n",
    "recalls = pickle.load( open( 'c_recall', \"rb\" ) )\n",
    "stories = pickle.load( open( 'c_stories', \"rb\" ) )\n",
    "\n",
    "gran_stories = pickle.load( open( 'gran_stories', \"rb\" ) )\n",
    "gran_templates = pickle.load( open( 'gran_templates', \"rb\" ) )\n",
    "# sums\n",
    "sums = pickle.load( open( \"new_sums\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Finding the 3 Top Ten Words for each story\n",
    "## A. Story Top Ten Words\n",
    "### 1. Get the residuals of each participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_residuals = {}\n",
    "\n",
    "for key in recalls:\n",
    "    ### Get the residuals\n",
    "    these_residuals = np.zeros((sums[key][0], 300))\n",
    "    count = 0\n",
    "    loc = key%10\n",
    "    soc = round(key/10)*10\n",
    "    # Make the inputs of the regression\n",
    "    # location template\n",
    "    l_temp = templates[loc].reshape(templates[loc].shape[0],-1)\n",
    "    # social template\n",
    "    s_temp = templates[soc].reshape(templates[soc].shape[0],-1)\n",
    "    # Concatenating inputes\n",
    "    inputs = np.concatenate((l_temp, s_temp), axis = 1)\n",
    "    for i in range(0,3):\n",
    "        n = recalls[key][i].shape[0]\n",
    "        for j in range(0, n):\n",
    "            output = np.copy(recalls[key][i][j])\n",
    "            #output = output.reshape(output.shape[0],-1)\n",
    "            model = LinearRegression().fit(inputs, output)\n",
    "            ## Is subtracting the intercept correct?? \n",
    "            new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "            these_residuals[count, :] = new_vec\n",
    "            count += 1\n",
    "    all_residuals[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For each participant correlate their residuals with each word in the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corrs = {}\n",
    "\n",
    "for key in recalls:\n",
    "    # turn key into a string\n",
    "    str_key = str(key)\n",
    "    # get the number of words\n",
    "    n_words = gran_stories[str_key]['wvs'].shape[0]\n",
    "    # get the number of participants\n",
    "    n_participants = all_residuals[key].shape[0]\n",
    "    # instantiate the vector n_words x n_participants\n",
    "    corr_vals = np.zeros((n_words, n_participants))\n",
    "    # Iterate through all words\n",
    "    for word in range(0, n_words):\n",
    "        # retrieve and reshape the vector for the word\n",
    "        this_word = gran_stories[str_key]['wvs'][word]\n",
    "        this_word = this_word.reshape(this_word.shape[0],-1)\n",
    "        # Iterate through each participant\n",
    "        for participant in range(0,n_participants):\n",
    "            # Retrieve and reshape the vector for the participants recall\n",
    "            this_partc = all_residuals[key][participant, :]\n",
    "            this_partc = this_partc.reshape(this_partc.shape[0],-1)\n",
    "            # Correlate the word and the participant recall\n",
    "            this_correl = np.corrcoef(this_word.T, this_partc.T)\n",
    "            # Store in corr_vals vector\n",
    "            corr_vals[word][participant] = this_correl[0,1]\n",
    "    # store corr_vals in dict\n",
    "    all_corrs[key] = corr_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Avg (or would it be better to sum) across all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avgs = {}\n",
    "all_sums = {}\n",
    "\n",
    "for key in all_corrs:\n",
    "    avg_vals = np.mean(all_corrs[key], axis = 1)\n",
    "    sum_vals = np.sum(all_corrs[key], axis = 1)\n",
    "    all_avgs[key] = avg_vals\n",
    "    all_sums[key] = sum_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Index the 10 largest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[ 46  21 116 635 656 544 577 135 103 440] [ 46  21 116 635 656 544 577 135 103 440]\n",
      "33\n",
      "[676 337 693 191  94 378 268 353 225 360] [676 337 693 191  94 378 268 353 225 360]\n",
      "44\n",
      "[399 500 471 497 259 279 236 263 254 392] [399 500 471 497 259 279 236 263 254 392]\n",
      "23\n",
      "[ 75  97 302 590 145 119 559 113 385 494] [ 75  97 302 590 145 119 559 113 385 494]\n",
      "12\n",
      "[430 114 176 484 360 376  39 155 280 460] [430 114 176 484 360 376  39 155 280 460]\n",
      "21\n",
      "[  0 446 181 390 252 272 532 436 488  29] [  0 446 181 390 252 272 532 436 488  29]\n",
      "13\n",
      "[403 437 126 361 539 337 180 423 103  35] [403 437 126 361 539 337 180 423 103  35]\n",
      "42\n",
      "[ 99 339 415   2 399 665 611 548 172 584] [ 99 339 415   2 399 665 611 548 172 584]\n",
      "43\n",
      "[460  49  19  28 433  74 159 355 423 339] [460  49  19  28 433  74 159 355 423 339]\n",
      "32\n",
      "[265 179 190  53 197 619 397 368 298 435] [265 179 190  53 197 619 397 368 298 435]\n",
      "34\n",
      "[ 91 129  32   2 514 235  65 477 354 231] [ 91 129  32   2 514 235  65 477 354 231]\n",
      "22\n",
      "[642  38 273 716 734 142 137  86  52  46] [642  38 273 716 734 142 137  86  52  46]\n",
      "41\n",
      "[325 328 237   5  15  23 253 436 100 372] [325 328 237   5  15  23 253 436 100 372]\n",
      "14\n",
      "[480 528 152   5 361 543 584  78 398 403] [480 528 152   5 361 543 584  78 398 403]\n",
      "24\n",
      "[161   0 449 427 397 379 354 336 581 249] [161   0 449 427 397 379 354 336 581 249]\n",
      "31\n",
      "[505 537 660 337 407 379 365 360 358 350] [505 537 660 337 407 379 365 360 358 350]\n"
     ]
    }
   ],
   "source": [
    "all_indices = {}\n",
    "\n",
    "for key in all_avgs:\n",
    "    print(key)\n",
    "    this_corr = all_avgs[key]\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    story_index_sums = all_sums[key].argsort()[-10:][::-1]\n",
    "    print(story_index, story_index_sums)\n",
    "    all_indices[key] = story_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get the Top Ten Words for the Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['he', 'he', 'he', 'he', 'he', 'he', 'she', 'she', 'she', 'she']\n",
      "33\n",
      "['bianci', 'cheese', 'cheese', 'cheese', 'cheese', 'cheese', 'cheeses', 'cheeses', 'cheeses', 'cheeses']\n",
      "44\n",
      "['headshe', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she']\n",
      "23\n",
      "['thanksgivin', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe']\n",
      "12\n",
      "['intolerability', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they']\n",
      "21\n",
      "['alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira']\n",
      "13\n",
      "['emmanuella', 'emmanuella', 'emmanuella', 'emmanuella', 'emmanuella', 'lineliam', 'emmanuella', 'emmanuella', 'emmanuella', 'emmanuella']\n",
      "42\n",
      "['anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna']\n",
      "43\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "32\n",
      "['to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to']\n",
      "34\n",
      "['fund', 'alma', 'alma', 'alma', 'alma', 'alma', 'alma', 'alma', 'alma', '.']\n",
      "22\n",
      "['fived', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had']\n",
      "41\n",
      "['deathstalker', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she']\n",
      "14\n",
      "['socratics', 'maria', 'maria', 'maria', 'maria', 'maria', 'maria', 'maria', 'maria', 'she']\n",
      "24\n",
      "['chemistrythis', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana']\n",
      "31\n",
      "['techjet', 'merger', 'merging', '.', '.', '.', '.', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "for key in all_indices:\n",
    "    print(key)\n",
    "    str_key = str(key)\n",
    "    this_index = all_indices[key]\n",
    "    top_words = []\n",
    "    for i in range(0, 10):\n",
    "        top_words.append(gran_stories[str_key]['words'][this_index[i]])\n",
    "    print(top_words)\n",
    "    top_ten[key] = {'story': top_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Location Top Ten Words\n",
    "### 1. Get the residuals of each participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_residuals = {}\n",
    "\n",
    "for key in recalls:\n",
    "    ### Get the residuals\n",
    "    these_residuals = np.zeros((sums[key][0], 300))\n",
    "    count = 0\n",
    "    soc = round(key/10)*10\n",
    "    # Make the inputs of the regression\n",
    "    # story vector\n",
    "    story_vec = stories[key].reshape(stories[key].shape[0],-1)\n",
    "    # social template\n",
    "    s_temp = templates[soc].reshape(templates[soc].shape[0],-1)\n",
    "    # Concatenating inputes\n",
    "    inputs = np.concatenate((story_vec, s_temp), axis = 1)\n",
    "    for i in range(0,3):\n",
    "        n = recalls[key][i].shape[0]\n",
    "        for j in range(0, n):\n",
    "            output = np.copy(recalls[key][i][j])\n",
    "            #output = output.reshape(output.shape[0],-1)\n",
    "            model = LinearRegression().fit(inputs, output)\n",
    "            ## Is subtracting the intercept correct?? \n",
    "            new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "            these_residuals[count, :] = new_vec\n",
    "            count += 1\n",
    "    all_residuals[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For each participant correlate their residuals with each word in the location template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corrs = {}\n",
    "\n",
    "for key in recalls:\n",
    "    # get the loc schema\n",
    "    loc = key%10\n",
    "    # get the number of words\n",
    "    n_words = gran_templates[loc]['wvs'].shape[0]\n",
    "    # get the number of participants\n",
    "    n_participants = all_residuals[key].shape[0]\n",
    "    # instantiate the vector n_words x n_participants\n",
    "    corr_vals = np.zeros((n_words, n_participants))\n",
    "    # Iterate through all words\n",
    "    for word in range(0, n_words):\n",
    "        # retrieve and reshape the vector for the word\n",
    "        this_word = gran_templates[loc]['wvs'][word]\n",
    "        this_word = this_word.reshape(this_word.shape[0],-1)\n",
    "        # Iterate through each participant\n",
    "        for participant in range(0,n_participants):\n",
    "            # Retrieve and reshape the vector for the participants recall\n",
    "            this_partc = all_residuals[key][participant, :]\n",
    "            this_partc = this_partc.reshape(this_partc.shape[0],-1)\n",
    "            # Correlate the word and the participant recall\n",
    "            this_correl = np.corrcoef(this_word.T, this_partc.T)\n",
    "            # Store in corr_vals vector\n",
    "            corr_vals[word][participant] = this_correl[0,1]\n",
    "    # store corr_vals in dict\n",
    "    all_corrs[key] = corr_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Avg (or would it be better to sum) across all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avgs = {}\n",
    "all_sums = {}\n",
    "\n",
    "for key in all_corrs:\n",
    "    avg_vals = np.mean(all_corrs[key], axis = 1)\n",
    "    sum_vals = np.sum(all_corrs[key], axis = 1)\n",
    "    all_avgs[key] = avg_vals\n",
    "    all_sums[key] = sum_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Index the 10 largest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[14  0  6  1 26  4  2 22 12 24] [14  0  6  1 26  4  2 22 12 24]\n",
      "33\n",
      "[35 17 23  1  9  3  8 22  0  2] [35 17 23  1  9  3  8 22  0  2]\n",
      "44\n",
      "[24 51 61 37 45 58 30 13 16 19] [24 51 61 37 45 58 30 13 16 19]\n",
      "23\n",
      "[ 1 23  3  9  0  8  2 22 18 54] [ 1 23  3  9  0  8  2 22 18 54]\n",
      "12\n",
      "[27 11 53 16 47 45 19 32 29 59] [27 11 53 16 47 45 19 32 29 59]\n",
      "21\n",
      "[ 0  1 26  6 30 24 31 29 17 27] [ 0  1 26  6 30 24 31 29 17 27]\n",
      "13\n",
      "[35 32 43 10 34 54 18 45 29 60] [35 32 43 10 34 54 18 45 29 60]\n",
      "42\n",
      "[27 12 14 16 32 31  2 10  1  6] [27 12 14 16 32 31  2 10  1  6]\n",
      "43\n",
      "[54 18  8 22  0  2  3  9 23  1] [54 18  8 22  0  2  3  9 23  1]\n",
      "32\n",
      "[11 53 20 42 60 30 45  8 59 29] [11 53 20 42 60 30 45  8 59 29]\n",
      "34\n",
      "[ 0 49  7 26 42 47 64 14 34 48] [ 0 49  7 26 42 47 64 14 34 48]\n",
      "22\n",
      "[34 45 47 27 28 53 11 40 16 55] [34 45 47 27 28 53 11 40 16 55]\n",
      "41\n",
      "[27 15  4 17 29  0  1 26  6 25] [27 15  4 17 29  0  1 26  6 25]\n",
      "14\n",
      "[ 0 49  7 26 42 14 47  2 13 58] [ 0 49  7 26 42 14 47  2 13 58]\n",
      "24\n",
      "[ 0 49 26 42  7 55 14  2 33  5] [ 0 49 26 42  7 55 14  2 33  5]\n",
      "31\n",
      "[20  5 10 23 15 29  0  1 26  6] [20  5 10 23 15 29  0  1 26  6]\n"
     ]
    }
   ],
   "source": [
    "all_indices = {}\n",
    "\n",
    "for key in all_avgs:\n",
    "    print(key)\n",
    "    this_corr = all_avgs[key]\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    story_index_sums = all_sums[key].argsort()[-10:][::-1]\n",
    "    print(story_index, story_index_sums)\n",
    "    all_indices[key] = story_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get the Top Ten Words for the Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['does', 'restaurant', 'restaurant', 'restaurant', 'restaurant', 'is', 'critic', 'like', 'like', 'food']\n",
      "['buy', 'purchasing', 'store', 'store', 'store', 'store', 'grocery', 'grocery', 'grocery', 'grocery']\n",
      "['is', 'is', 'is', 'is', 'is', 'class', 'class', 'class', 'class', 'class']\n",
      "['store', 'store', 'store', 'store', 'grocery', 'grocery', 'grocery', 'grocery', 'groceries', 'groceries']\n",
      "['they', 'to', 'and', 'when', 'are', 'which', 'arrive', 'departs', 'until', 'in']\n",
      "['restaurant', 'restaurant', 'restaurant', 'restaurant', 'food', 'food', 'food', 'ordering', 'order', 'being']\n",
      "['buy', 'pick', 'which', 'picking', 'to', 'groceries', 'groceries', 'do', 'do', 'do']\n",
      "['they', 'boarding', 'boarding', 'when', 'departs', 'flight', 'customer', 'getting', 'airport', 'airport']\n",
      "['groceries', 'groceries', 'grocery', 'grocery', 'grocery', 'grocery', 'store', 'store', 'store', 'store']\n",
      "['to', 'and', 'at', 'at', 'on', 'their', 'which', 'through', 'in', 'until']\n",
      "['lecture', 'lecture', 'lecture', 'lecture', 'lecture', 'taught', 'due', 'teaching', 'in', 'in']\n",
      "['is', 'which', 'are', 'they', 'have', 'and', 'to', '-', 'when', 'does']\n",
      "['being', 'each', 'is', 'order', 'ordering', 'restaurant', 'restaurant', 'restaurant', 'restaurant', 'entering']\n",
      "['lecture', 'lecture', 'lecture', 'lecture', 'lecture', 'teaching', 'taught', 'dean', 'class', 'class']\n",
      "['lecture', 'lecture', 'lecture', 'lecture', 'lecture', 'assignment', 'teaching', 'dean', 'students', 'studies']\n",
      "['the', 'the', 'the', 'the', 'each', 'ordering', 'restaurant', 'restaurant', 'restaurant', 'restaurant']\n"
     ]
    }
   ],
   "source": [
    "for key in all_indices:\n",
    "    # get the loc schema\n",
    "    loc = key%10\n",
    "    this_index = all_indices[key]\n",
    "    top_words = []\n",
    "    for i in range(0, 10):\n",
    "        top_words.append(gran_templates[loc]['words'][this_index[i]])\n",
    "    print(top_words)\n",
    "    top_ten[key]['loc'] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Social Top Ten Words\n",
    "### 1. Get the residuals of each participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_residuals = {}\n",
    "\n",
    "for key in recalls:\n",
    "    ### Get the residuals\n",
    "    these_residuals = np.zeros((sums[key][0], 300))\n",
    "    count = 0\n",
    "    soc = round(key/10)*10\n",
    "    # Make the inputs of the regression\n",
    "    # story vector\n",
    "    story_vec = stories[key].reshape(stories[key].shape[0],-1)\n",
    "    # location template\n",
    "    l_temp = templates[loc].reshape(templates[loc].shape[0],-1)\n",
    "    # Concatenating inputes\n",
    "    inputs = np.concatenate((story_vec, l_temp), axis = 1)\n",
    "    for i in range(0,3):\n",
    "        n = recalls[key][i].shape[0]\n",
    "        for j in range(0, n):\n",
    "            output = np.copy(recalls[key][i][j])\n",
    "            #output = output.reshape(output.shape[0],-1)\n",
    "            model = LinearRegression().fit(inputs, output)\n",
    "            ## Is subtracting the intercept correct?? \n",
    "            new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "            these_residuals[count, :] = new_vec\n",
    "            count += 1\n",
    "    all_residuals[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For each participant correlate their residuals with each word in the location template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corrs = {}\n",
    "\n",
    "for key in recalls:\n",
    "    # get the soc schema\n",
    "    soc = round(key/10)*10\n",
    "    # get the number of words\n",
    "    n_words = gran_templates[soc]['wvs'].shape[0]\n",
    "    # get the number of participants\n",
    "    n_participants = all_residuals[key].shape[0]\n",
    "    # instantiate the vector n_words x n_participants\n",
    "    corr_vals = np.zeros((n_words, n_participants))\n",
    "    # Iterate through all words\n",
    "    for word in range(0, n_words):\n",
    "        # retrieve and reshape the vector for the word\n",
    "        this_word = gran_templates[soc]['wvs'][word]\n",
    "        this_word = this_word.reshape(this_word.shape[0],-1)\n",
    "        # Iterate through each participant\n",
    "        for participant in range(0,n_participants):\n",
    "            # Retrieve and reshape the vector for the participants recall\n",
    "            this_partc = all_residuals[key][participant, :]\n",
    "            this_partc = this_partc.reshape(this_partc.shape[0],-1)\n",
    "            # Correlate the word and the participant recall\n",
    "            this_correl = np.corrcoef(this_word.T, this_partc.T)\n",
    "            # Store in corr_vals vector\n",
    "            corr_vals[word][participant] = this_correl[0,1]\n",
    "    # store corr_vals in dict\n",
    "    all_corrs[key] = corr_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Avg (or would it be better to sum) across all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avgs = {}\n",
    "all_sums = {}\n",
    "\n",
    "for key in all_corrs:\n",
    "    avg_vals = np.mean(all_corrs[key], axis = 1)\n",
    "    sum_vals = np.sum(all_corrs[key], axis = 1)\n",
    "    all_avgs[key] = avg_vals\n",
    "    all_sums[key] = sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Index the 10 largest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[48 78 49 53 22 56 77 35 34 57] [48 78 49 53 22 56 77 35 34 57]\n",
      "33\n",
      "[53  1 14 44  2 18  0 49 29 26] [53  1 14 44  2 18  0 49 29 26]\n",
      "44\n",
      "[33 32 59 24 40 66 38 45 51 31] [33 32 59 24 40 66 38 45 51 31]\n",
      "23\n",
      "[48  0  1 22 20 29 42  9 30 13] [48  0  1 22 20 29 42  9 30 13]\n",
      "12\n",
      "[56 77 13  9 63 49 58 66 61 33] [56 77 13  9 63 49 58 66 61 33]\n",
      "21\n",
      "[20 44 48  0  1 10  5 18  3 21] [20 44 48  0  1 10  5 18  3 21]\n",
      "13\n",
      "[78 56 77 53 64 10 74 48 49 51] [78 56 77 53 64 10 74 48 49 51]\n",
      "42\n",
      "[33 59 50 70 58 45 38 68 51 31] [33 59 50 70 58 45 38 68 51 31]\n",
      "43\n",
      "[ 7 61 33 60 68 31 51 19 70 50] [ 7 61 33 60 68 31 51 19 70 50]\n",
      "32\n",
      "[44 53  1 14 31 52 48  6 37 13] [44 53  1 14 31 52 48  6 37 13]\n",
      "34\n",
      "[53 14  1 26 12 30 28 33  8 44] [53 14  1 26 12 30 28 33  8 44]\n",
      "22\n",
      "[40 32 18 44 21 26 10 13 30 43] [40 32 18 44 21 26 10 13 30 43]\n",
      "41\n",
      "[33 31 68 51 32 44 63 19 59 38] [33 31 68 51 32 44 63 19 59 38]\n",
      "14\n",
      "[49 35 48 78 87  5 14 18 27  0] [49 35 48 78 87  5 14 18 27  0]\n",
      "24\n",
      "[48 20  5 22  0  1 44  2 36 10] [48 20  5 22  0  1 44  2 36 10]\n",
      "31\n",
      "[44 53  1 14 37  6 48 13 52 40] [44 53  1 14 37  6 48 13 52 40]\n"
     ]
    }
   ],
   "source": [
    "all_indices = {}\n",
    "\n",
    "for key in all_avgs:\n",
    "    print(key)\n",
    "    this_corr = all_avgs[key]\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    story_index_sums = all_sums[key].argsort()[-10:][::-1]\n",
    "    print(story_index, story_index_sums)\n",
    "    all_indices[key] = story_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Get the Top Ten Words for the Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'wants', 'she', 'does', 'has', 'who', 'who', 'her', 'his', 'is']\n",
      "['deal', 'deal', 'deal', 'with', 'business', 'business', 'business', 'business', 'stake', 'money']\n",
      "['she', 'he', 'meets', 'is', 'is', 'is', 'person', 'begins', 'when', 'when']\n",
      "['fiancee', 'wedding', 'wedding', 'dating', 'couple', 'ring', 'ring', 'ring', 'and', 'and']\n",
      "['who', 'who', 'to', 'to', 'to', 'she', 'being', 'and', 'with', 'with']\n",
      "['couple', 'who', 'fiancee', 'wedding', 'wedding', 'proposing', 'proposal', 'has', 'preparing', 'been']\n",
      "['wants', 'who', 'who', 'does', 'break', 'break', 'them', 'he', 'she', 'breaking']\n",
      "['she', 'meets', 'couple', 'couple', 'couple', 'begins', 'person', 'when', 'when', 'when']\n",
      "['for', 'for', 'she', 'and', 'when', 'when', 'when', 'later', 'couple', 'couple']\n",
      "['with', 'deal', 'deal', 'deal', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "['deal', 'deal', 'deal', 'money', 'concluding', 'in', 'at', 'offer', 'offer', 'with']\n",
      "['is', 'is', 'has', 'who', 'been', 'are', 'proposing', 'and', 'and', 'presented']\n",
      "['she', 'when', 'when', 'when', 'he', 'that', 'occasion', 'later', 'meets', 'person']\n",
      "['she', 'her', 'he', 'wants', 'breakup', 'breakup', 'breakup', 'breakup', 'breakup', 'breakup']\n",
      "['fiancee', 'couple', 'proposal', 'dating', 'wedding', 'wedding', 'who', 'planner', 'in', 'proposing']\n",
      "['with', 'deal', 'deal', 'deal', 'the', 'the', 'the', 'the', 'the', 'the']\n"
     ]
    }
   ],
   "source": [
    "for key in all_indices:\n",
    "    # get the loc schema\n",
    "    soc = round(key/10)*10\n",
    "    this_index = all_indices[key]\n",
    "    top_words = []\n",
    "    for i in range(0, 10):\n",
    "        top_words.append(gran_templates[soc]['words'][this_index[i]])\n",
    "    print(top_words)\n",
    "    top_ten[key]['soc'] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Presenting Top Ten Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "story\n",
      "['he', 'he', 'he', 'he', 'he', 'he', 'she', 'she', 'she', 'she']\n",
      "loc\n",
      "['does', 'restaurant', 'restaurant', 'restaurant', 'restaurant', 'is', 'critic', 'like', 'like', 'food']\n",
      "soc\n",
      "['he', 'wants', 'she', 'does', 'has', 'who', 'who', 'her', 'his', 'is']\n",
      "33\n",
      "story\n",
      "['bianci', 'cheese', 'cheese', 'cheese', 'cheese', 'cheese', 'cheeses', 'cheeses', 'cheeses', 'cheeses']\n",
      "loc\n",
      "['buy', 'purchasing', 'store', 'store', 'store', 'store', 'grocery', 'grocery', 'grocery', 'grocery']\n",
      "soc\n",
      "['deal', 'deal', 'deal', 'with', 'business', 'business', 'business', 'business', 'stake', 'money']\n",
      "44\n",
      "story\n",
      "['headshe', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she']\n",
      "loc\n",
      "['is', 'is', 'is', 'is', 'is', 'class', 'class', 'class', 'class', 'class']\n",
      "soc\n",
      "['she', 'he', 'meets', 'is', 'is', 'is', 'person', 'begins', 'when', 'when']\n",
      "23\n",
      "story\n",
      "['thanksgivin', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe', 'chloe']\n",
      "loc\n",
      "['store', 'store', 'store', 'store', 'grocery', 'grocery', 'grocery', 'grocery', 'groceries', 'groceries']\n",
      "soc\n",
      "['fiancee', 'wedding', 'wedding', 'dating', 'couple', 'ring', 'ring', 'ring', 'and', 'and']\n",
      "12\n",
      "story\n",
      "['intolerability', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they']\n",
      "loc\n",
      "['they', 'to', 'and', 'when', 'are', 'which', 'arrive', 'departs', 'until', 'in']\n",
      "soc\n",
      "['who', 'who', 'to', 'to', 'to', 'she', 'being', 'and', 'with', 'with']\n",
      "21\n",
      "story\n",
      "['alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira', 'alvira']\n",
      "loc\n",
      "['restaurant', 'restaurant', 'restaurant', 'restaurant', 'food', 'food', 'food', 'ordering', 'order', 'being']\n",
      "soc\n",
      "['couple', 'who', 'fiancee', 'wedding', 'wedding', 'proposing', 'proposal', 'has', 'preparing', 'been']\n",
      "13\n",
      "story\n",
      "['emmanuella', 'emmanuella', 'emmanuella', 'emmanuella', 'emmanuella', 'lineliam', 'emmanuella', 'emmanuella', 'emmanuella', 'emmanuella']\n",
      "loc\n",
      "['buy', 'pick', 'which', 'picking', 'to', 'groceries', 'groceries', 'do', 'do', 'do']\n",
      "soc\n",
      "['wants', 'who', 'who', 'does', 'break', 'break', 'them', 'he', 'she', 'breaking']\n",
      "42\n",
      "story\n",
      "['anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna', 'anna']\n",
      "loc\n",
      "['they', 'boarding', 'boarding', 'when', 'departs', 'flight', 'customer', 'getting', 'airport', 'airport']\n",
      "soc\n",
      "['she', 'meets', 'couple', 'couple', 'couple', 'begins', 'person', 'when', 'when', 'when']\n",
      "43\n",
      "story\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "loc\n",
      "['groceries', 'groceries', 'grocery', 'grocery', 'grocery', 'grocery', 'store', 'store', 'store', 'store']\n",
      "soc\n",
      "['for', 'for', 'she', 'and', 'when', 'when', 'when', 'later', 'couple', 'couple']\n",
      "32\n",
      "story\n",
      "['to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to']\n",
      "loc\n",
      "['to', 'and', 'at', 'at', 'on', 'their', 'which', 'through', 'in', 'until']\n",
      "soc\n",
      "['with', 'deal', 'deal', 'deal', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "34\n",
      "story\n",
      "['fund', 'alma', 'alma', 'alma', 'alma', 'alma', 'alma', 'alma', 'alma', '.']\n",
      "loc\n",
      "['lecture', 'lecture', 'lecture', 'lecture', 'lecture', 'taught', 'due', 'teaching', 'in', 'in']\n",
      "soc\n",
      "['deal', 'deal', 'deal', 'money', 'concluding', 'in', 'at', 'offer', 'offer', 'with']\n",
      "22\n",
      "story\n",
      "['fived', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had']\n",
      "loc\n",
      "['is', 'which', 'are', 'they', 'have', 'and', 'to', '-', 'when', 'does']\n",
      "soc\n",
      "['is', 'is', 'has', 'who', 'been', 'are', 'proposing', 'and', 'and', 'presented']\n",
      "41\n",
      "story\n",
      "['deathstalker', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she', 'she']\n",
      "loc\n",
      "['being', 'each', 'is', 'order', 'ordering', 'restaurant', 'restaurant', 'restaurant', 'restaurant', 'entering']\n",
      "soc\n",
      "['she', 'when', 'when', 'when', 'he', 'that', 'occasion', 'later', 'meets', 'person']\n",
      "14\n",
      "story\n",
      "['socratics', 'maria', 'maria', 'maria', 'maria', 'maria', 'maria', 'maria', 'maria', 'she']\n",
      "loc\n",
      "['lecture', 'lecture', 'lecture', 'lecture', 'lecture', 'teaching', 'taught', 'dean', 'class', 'class']\n",
      "soc\n",
      "['she', 'her', 'he', 'wants', 'breakup', 'breakup', 'breakup', 'breakup', 'breakup', 'breakup']\n",
      "24\n",
      "story\n",
      "['chemistrythis', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana', 'lana']\n",
      "loc\n",
      "['lecture', 'lecture', 'lecture', 'lecture', 'lecture', 'assignment', 'teaching', 'dean', 'students', 'studies']\n",
      "soc\n",
      "['fiancee', 'couple', 'proposal', 'dating', 'wedding', 'wedding', 'who', 'planner', 'in', 'proposing']\n",
      "31\n",
      "story\n",
      "['techjet', 'merger', 'merging', '.', '.', '.', '.', '.', '.', '.']\n",
      "loc\n",
      "['the', 'the', 'the', 'the', 'each', 'ordering', 'restaurant', 'restaurant', 'restaurant', 'restaurant']\n",
      "soc\n",
      "['with', 'deal', 'deal', 'deal', 'the', 'the', 'the', 'the', 'the', 'the']\n"
     ]
    }
   ],
   "source": [
    "for story in top_ten:\n",
    "    print(story)\n",
    "    for keys in top_ten[story]:\n",
    "        print(keys)\n",
    "        print(top_ten[story][keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
