{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Top Ten Words of Recall words for each wv\n",
    "## \"Third\" bc this is the third notebook that does this, but in this one, we regress out different things (29):\n",
    "### L_corr = corr(L, recall - story)\n",
    "### S_corr = corr(S, recall - story)\n",
    "### story_corr = corr(story, recall - L - S - other stories with shared schema)\n",
    "### ) all other possible regressors besides the regressor used in the correlation are used in the initial regression\n",
    "\n",
    "## 2/18/20\n",
    "### For each story there will be 3 'Top Ten Words', One for each type of regressor\n",
    "#### 1) Concatenate unique words across all participants\n",
    "#### 2) For each regressor, regress out of the words the other vector(s). \n",
    "#### 3) Correlate the residual wvs with the target regressor. \n",
    "#### 4) Index the 10 highest values, and find the 10 highest words\n",
    "#### 5) Put in dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import copy\n",
    "from random import randrange\n",
    "from sklearn.metrics import jaccard_score\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA #for cluster analysis\n",
    "from gensim.models import KeyedVectors #for word embeddings\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "import os #for importing\n",
    "import pickle #for loading transcripts\n",
    "from scipy.stats import pearsonr \n",
    "\n",
    "# from _DRAFT_20200604_functions import * #includes constants and score function\n",
    "from tqdm import tqdm_notebook #for progress bar\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import recalls, and uncentered story and template vectors and sums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import recalls and sums\n",
    "recalls = pickle.load( open( 'fr_recalls', \"rb\" ) )\n",
    "sums = pickle.load( open( \"fr_sums\", \"rb\" ) )\n",
    "\n",
    "#import non-centered story, template\n",
    "templates = pickle.load( open( 'template_vectors', \"rb\" ) )\n",
    "stories = pickle.load( open( 'actual_story_vectors', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipath = 'rolando/wiki-news-300d-1M.vec'\n",
    "wv_model = KeyedVectors.load_word2vec_format(wikipath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = 300\n",
    "\n",
    "# FastText preprocessing, based on bittlingmayer/ft_wiki_preproc.py\n",
    "# Remove special characters, put spaces between all tokens\n",
    "SUB = [\"s/’/'/g\", \"s/′/'/g\", \"s/''/ /g\", \"s/'/ ' /g\", 's/“/\"/g', 's/”/\"/g', 's/\"/ /g', \"s/\\\\./ \\\\. /g\", \"s/<br \\\\/>/ /g\", \"s/, / , /g\", \"s/(/ ( /g\", \"s/)/ ) /g\", \"s/\\\\!/ \\\\! /g\", \"s/\\\\?/ \\\\? /g\", \"s/\\\\;/ /g\", \"s/\\\\:/ /g\", \"s/-/ - /g\", \"s/=/ /g\", \"s/=/ /g\", \"s/*/ /g\", \"s/|/ /g\", \"s/«/ /g\", \n",
    "       \"s/…/ /g\", \"s/‘/ /g\", \"s/í/ /g\", \"s/ñ/ /g\", \"s/\\x84/ /g\", \"s/î/ /g\", \"s/ó/ /g\", \"s/\\x83/ /g\", \"s/ï/ /g\", \"s/õ/ /g\",\n",
    "       \"s/ò/ /g\", \"s/,/ /g\", \"s/ô/ /g\", \"s/\\x92/ /g\", \"s/é/ /g\", \"s/\\x8e/ /g\", \"s/â\\x80¦/ /g\", \"s/\\x91/ /g\", \"s/\\x93/ /g\",\n",
    "       \"s/\\x94/ /g\", \"s/ã®/ /g\", \"s/ã¨/ /g\", \"s/ã©/ /g\",\n",
    "       \"s/\\â\\x80\\x99/ /g\", \"s/â\\x80\\x9c/ /g\", \"s/â\\x80\\x9d/ /g\", \"s/â\\x80\\x99/ /g\", \"s/â\\x80\\x9c/ /g\", \"s/â\\x80\\x98/ /g\",\n",
    "       \"s/â/ /g\"]\n",
    "\n",
    "def __normalize_text(s):\n",
    "    for sg in SUB:\n",
    "        rep = sg.replace('\\\\','').split('/')\n",
    "        s = s.replace(rep[1], rep[2])\n",
    "    s = s.replace('/',' ')\n",
    "    return s\n",
    "\n",
    "def __spaces(s):\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def __digits(s):\n",
    "    return ''.join(filter(lambda c: not c.isdigit(), s))\n",
    "\n",
    "# def preproc(s):\n",
    "#     return __punctuation(__spaces(__digits(__normalize_text(s.lower()))))\n",
    "\n",
    "def preproc(s):\n",
    "    return (__spaces(__digits(__normalize_text(s.lower()))))\n",
    "\n",
    "def __punctuation(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def word2vecSent(sentence, model = 'fasttext'):\n",
    "    wv_dim = 300 #for glove and fasttext\n",
    "    \n",
    "    if model == 'glove':\n",
    "        wvmodel = glove_model\n",
    "    elif model == 'fasttext':\n",
    "        wvmodel = wv_model\n",
    "        \n",
    "    words = preproc(sentence).split(' ')\n",
    "    wv = np.zeros((len(words), wv_dim))\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in wvmodel.vocab:\n",
    "            wv[i,:] = wvmodel.word_vec(words[i])\n",
    "    \n",
    "    return words, wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. 3 sets of top ten \n",
    "## A. Find unique words in recalls of each story\n",
    "### 1. Concatenate words and word vectors in parallel across all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = {}\n",
    "all_wvs = {}\n",
    "\n",
    "#iterate through all recalls in a story and concatente words and wvs of all stories\n",
    "for key in recalls:\n",
    "    words = np.zeros((0,1))\n",
    "    wvs = np.zeros((0,300))\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,len(recalls[key][i])):\n",
    "            p_words, p_wvs = word2vecSent(recalls[key][i][j][0])\n",
    "            # reshape p_words\n",
    "            p_words = np.array(p_words)\n",
    "            p_words = p_words.reshape(p_words.shape[0],-1)\n",
    "            # stack p_words and p_wvs\n",
    "            words = np.vstack((words, p_words))\n",
    "            wvs = np.vstack((wvs, p_wvs))\n",
    "    all_words[key] = words\n",
    "    all_wvs[key] = wvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Centering all words + story wvs + template wvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all words from recall\n",
    "\n",
    "concat = np.zeros((0,300))\n",
    "\n",
    "for key in all_wvs:\n",
    "    for i in range(0, len(all_wvs[key])):\n",
    "        concat = np.vstack((concat, all_wvs[key][i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37545, 300)\n"
     ]
    }
   ],
   "source": [
    "print(concat.shape)\n",
    "for key in stories:\n",
    "    concat = np.vstack((concat, stories[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37561, 300)\n",
      "(37569, 300)\n"
     ]
    }
   ],
   "source": [
    "print(concat.shape)\n",
    "for key in templates:\n",
    "    concat = np.vstack((concat, templates[key]))\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "centering_vec = np.mean(concat, axis = 0)\n",
    "\n",
    "# template vectors\n",
    "\n",
    "for key in templates:\n",
    "    templates[key] = templates[key] - centering_vec\n",
    "    \n",
    "# recall vectors\n",
    "\n",
    "for key in recalls:\n",
    "    for i in range(0, len(all_wvs[key])):\n",
    "        all_wvs[key][i] = all_wvs[key][i] - centering_vec\n",
    "            \n",
    "#story vectors\n",
    "#make new dict with int key instead of string\n",
    "int_stories = {}\n",
    "for key in stories:\n",
    "    int_stories[int(key)] = stories[key] - centering_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find unique words and wvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_words = {}\n",
    "au_wvs = {}\n",
    "\n",
    "for key in all_words:\n",
    "    # find unique wvs and their indices:\n",
    "    unique_wvs, index = np.unique(all_wvs[key], axis=0, return_index=True)\n",
    "    unique_words =  all_words[key][index]\n",
    "    # put in dicts\n",
    "    au_wvs[key] = unique_wvs\n",
    "    au_words[key] = unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. *Story* Top Ten\n",
    "### A. Regress out of all the unique_wvs the loc and soc template wvs\n",
    "#### i. Concatenating the schema part of the matrix used as input to regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_order = [1, 2, 3, 4, 10, 20, 30, 40]\n",
    "\n",
    "this_input2 = np.zeros((300,0))\n",
    "\n",
    "for i in schema_order:\n",
    "    this_input2 = np.concatenate((this_input2, templates[i].reshape(templates[i].shape[0],-1)), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 8 regressors total: L + S templates + 6 schema sharing stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n",
      "(300, 17)\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    # add on all other stories to the regression input\n",
    "    loc = key%10\n",
    "    soc = round(key/10)*10\n",
    "    # location template\n",
    "    l_temp = templates[loc].reshape(templates[loc].shape[0],-1)\n",
    "    # social template\n",
    "    s_temp = templates[soc].reshape(templates[soc].shape[0],-1)\n",
    "    inputs = np.concatenate((l_temp, s_temp), axis = 1)\n",
    "    # List the set of stories that share a schema with the stories\n",
    "    shared_schema = []\n",
    "    for story in story_order:\n",
    "        this_loc = story%10\n",
    "        this_soc = round(story/10)*10\n",
    "        if story == key:\n",
    "            pass\n",
    "        elif this_loc == loc:\n",
    "            shared_schema.append(story)\n",
    "        elif this_soc == soc:\n",
    "            shared_schema.append(story)\n",
    "        #else:\n",
    "        #    shared_schema.append(story)\n",
    "    # Iterate through the list\n",
    "    for story in shared_schema:\n",
    "        inputs = np.concatenate((inputs, int_stories[story].reshape(int_stories[story].shape[0],-1)), axis = 1)\n",
    "    print(inputs.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(inputs, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 8 regressors total: only the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 8)\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    #inputs\n",
    "    inputs = np.copy(this_input2)\n",
    "    print(inputs.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(inputs, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 23 regressors: all possible regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    #inputs\n",
    "    inputs = np.copy(this_input2)\n",
    "    for story in story_order:\n",
    "        if story == key:\n",
    "            print('skip')\n",
    "        else:\n",
    "            inputs = np.concatenate((inputs, stories[str(story)].reshape(stories[str(story)].shape[0],-1)), axis = 1)\n",
    "    print(inputs.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(inputs, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 14 Regressors: The 8 schemas + 6 related stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n",
      "(300, 8)\n",
      "(300, 8)\n",
      "(300, 14)\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    #inputs\n",
    "    inputs = np.copy(this_input2)\n",
    "    print(inputs.shape)\n",
    "    loc = key%10\n",
    "    soc = round(key/10)*10\n",
    "    # List the set of stories that share a schema with the stories\n",
    "    shared_schema = []\n",
    "    for story in story_order:\n",
    "        this_loc = story%10\n",
    "        this_soc = round(story/10)*10\n",
    "        if story == key:\n",
    "            pass\n",
    "        elif this_loc == loc:\n",
    "            shared_schema.append(story)\n",
    "        elif this_soc == soc:\n",
    "            shared_schema.append(story)\n",
    "    # Iterate through the list\n",
    "    print(inputs.shape)\n",
    "    for story in shared_schema:\n",
    "        inputs = np.concatenate((inputs, int_stories[story].reshape(int_stories[story].shape[0],-1)), axis = 1)\n",
    "    print(inputs.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(inputs, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 15 regressors: only all the stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n",
      "skip\n",
      "(300, 15)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    #inputs\n",
    "    inputs = np.zeros((300,0))\n",
    "    for story in story_order:\n",
    "        if story == key:\n",
    "            print('skip')\n",
    "        else:\n",
    "            inputs = np.concatenate((inputs, stories[str(story)].reshape(stories[str(story)].shape[0],-1)), axis = 1)\n",
    "    print(inputs.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(inputs, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 9 Regressors: The stories that don't share a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n",
      "(300, 9)\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    #inputs\n",
    "    inputs = np.zeros((300,0))\n",
    "    loc = key%10\n",
    "    soc = round(key/10)*10\n",
    "    # List the set of stories that share a schema with the stories\n",
    "    shared_schema = []\n",
    "    for story in story_order:\n",
    "        this_loc = story%10\n",
    "        this_soc = round(story/10)*10\n",
    "        if story == key:\n",
    "            pass\n",
    "        elif this_loc == loc:\n",
    "            pass\n",
    "        elif this_soc == soc:\n",
    "            pass\n",
    "        else:\n",
    "            shared_schema.append(story)\n",
    "    # Iterate through the list\n",
    "    for story in shared_schema:\n",
    "        inputs = np.concatenate((inputs, int_stories[story].reshape(int_stories[story].shape[0],-1)), axis = 1)\n",
    "    print(inputs.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(inputs, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 17 Regressors: 15 stories + Loc and Soc Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n",
      "skip\n",
      "(300, 17)\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    # add on all other stories to the regression input\n",
    "    loc = key%10\n",
    "    soc = round(key/10)*10\n",
    "    # location template\n",
    "    l_temp = templates[loc].reshape(templates[loc].shape[0],-1)\n",
    "    # social template\n",
    "    s_temp = templates[soc].reshape(templates[soc].shape[0],-1)\n",
    "    inputs = np.concatenate((l_temp, s_temp), axis = 1)\n",
    "    # List the set of stories that share a schema with the stories\n",
    "    for story in story_order:\n",
    "        if story == key:\n",
    "            print('skip')\n",
    "        else:\n",
    "            inputs = np.concatenate((inputs, stories[str(story)].reshape(stories[str(story)].shape[0],-1)), axis = 1)\n",
    "    print(inputs.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(inputs, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, inputs.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Correlate each wv in the recall with the story wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correls = {}\n",
    "\n",
    "for key in resid_wvs:\n",
    "    corr = np.zeros((0,1))\n",
    "    this_story = stories[str(key)].reshape(stories[str(key)].shape[0], -1)\n",
    "    for i in range(0, len(resid_wvs[key])):\n",
    "        this_wv = resid_wvs[key][i].reshape(resid_wvs[key][i].shape[0], -1)\n",
    "        this_correl = np.corrcoef(this_story.T, this_wv.T)\n",
    "        # add to corr\n",
    "        corr = np.vstack((corr, this_correl[0,1]))\n",
    "    all_correls[key] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Top Ten values' indices and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['thomas' 'got' 't' \"'\" 'sat' 'hadn' 'ate' 'started' 'burguer' 'had']\n",
      "33\n",
      "['ms' 'cheeses' 'stores' 'cheese' 'product' 'products' 'morrison' '.'\n",
      " 'crackers' 'marketed']\n",
      "44\n",
      "['during' 'saw' 'egyptology' 'found' 'wrote' 'had' 'mummification' 'took'\n",
      " 'heard' 'looked']\n",
      "23\n",
      "['alex' 'chloe' 'loves' 'excitement' 'likes' 'chris' 'everyone' 'shoppers'\n",
      " 'he' 'day']\n",
      "12\n",
      "['calvin' 'they' 'ultimately' 'not' 'delayed' 'would' 'flight' 'was' 't'\n",
      " 'conveyor']\n",
      "21\n",
      "['senna' 'oysters' 'champagne' 'dishes' 'delicious' 'caramelized'\n",
      " 'scallops' 'tears' 'garlic' 'delight']\n",
      "13\n",
      "['tonight' 'think' 'going' 'well' 'however' 'agreeing' 'nodded' 'said'\n",
      " 'not' 'anyway']\n",
      "42\n",
      "['she' 'gave' 'anna' 'had' 'her' 'was' 'of' 'might' 'could' 'rate']\n",
      "43\n",
      "['carina' 'simon' 'ribs' 'meat' 'cute' 'chickens' 'blushed' 'hens'\n",
      " 'unlucky' 'met']\n",
      "32\n",
      "['jeff' 'william' 'wine' 'steep' 'pass' 'vineyard' 'per' 'd' 'samples'\n",
      " 'vineyards']\n",
      "34\n",
      "['alma' 'fund' 'funding' 'funds' 'investors' 'project' 'investor'\n",
      " 'capital' 'investment' 'finance']\n",
      "22\n",
      "['digging' 'belongings' 'back' 'away' 'onto' 'sven' 'their' 'duffle'\n",
      " 'into' 'dig']\n",
      "41\n",
      "['teresa' 'james' 'theresa' 'jessica' 'waitress' 'her' 'john' 'him' 'she'\n",
      " 'waiter']\n",
      "14\n",
      "['i' 'maria' 'henry' 'you' 'her' 'my' 'me' 'she' 'your' 'if']\n",
      "24\n",
      "['aaron' 'lana' 'protons' 'sodium' 'classmate' 'another' 'nucleus'\n",
      " 'number' 'one' 'ions']\n",
      "31\n",
      "['mr' '.' 'gordon' 'large' 'his' 'company' 'companies' 'small' 'smaller'\n",
      " 'industry']\n"
     ]
    }
   ],
   "source": [
    "for key in all_correls:\n",
    "    print(key)\n",
    "    this_corr = all_correls[key]\n",
    "    this_corr = np.ravel(this_corr)\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    #print(story_index)\n",
    "    # index the top words\n",
    "    top_words = np.zeros((0,1))\n",
    "    for i in range(0, 10):\n",
    "        top_words = np.vstack((top_words, au_words[key][story_index[i]]))\n",
    "    top_words = top_words.ravel()\n",
    "    print(top_words)\n",
    "    top_ten[key] = {'story': top_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. *Location* Top Ten\n",
    "### 1. Regress out of all the unique_wvs the story and soc template wvs\n",
    "#### a. Concatenating the story part of the matrix used as input to regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 16)\n"
     ]
    }
   ],
   "source": [
    "this_input = np.zeros((300,0))\n",
    "\n",
    "for story in story_order:\n",
    "    this_input = np.concatenate((this_input, stories[str(story)].reshape(stories[str(story)].shape[0],-1)), axis = 1)\n",
    "\n",
    "print(this_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    loc = key%10\n",
    "    # Make the inputs of the regression\n",
    "    new_input = int_stories[key].reshape(int_stories[key].shape[0],-1)        \n",
    "    print(new_input.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(new_input, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, new_input.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Correlate each wv in the recall with the story wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correls = {}\n",
    "\n",
    "for key in resid_wvs:\n",
    "    corr = np.zeros((0,1))\n",
    "    loc = key%10\n",
    "    this_loc = templates[loc].reshape(templates[loc].shape[0], -1)\n",
    "    for i in range(0, len(resid_wvs[key])):\n",
    "        this_wv = resid_wvs[key][i].reshape(resid_wvs[key][i].shape[0], -1)\n",
    "        this_correl = np.corrcoef(this_loc.T, this_wv.T)\n",
    "        # add to corr\n",
    "        corr = np.vstack((corr, this_correl[0,1]))\n",
    "    all_correls[key] = corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top Ten values' indices and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['restaurant' 'food' 'menus' 'menu' 'diner' 'restuarant' 'waiter' 'foods'\n",
      " 'meal' 'dessert']\n",
      "33\n",
      "['grocery' 'store' 'groceries' 'stores' 'supermarket' 'shop' 'checkout'\n",
      " 'checkouts' 'cashier' 'purchase']\n",
      "44\n",
      "['lecture' 'class' 'lectures' 'lecturing' 'assignment' 'hall' 'classroom'\n",
      " 'lesson' 'of' 'lecturer']\n",
      "23\n",
      "['grocery' 'store' 'groceries' 'supermarket' 'checkout' 'shop' 'shopping'\n",
      " 'checkouts' 'grocer' 'shoppers']\n",
      "12\n",
      "['airport' 'airports' 'plane' 'flight' 'boarding' 'gate' 'gates'\n",
      " 'security' 'departure' 'arriving']\n",
      "21\n",
      "['restaurant' 'food' 'menus' 'menu' 'chef' 'meal' 'dishes' 'waiter'\n",
      " 'ordering' 'eating']\n",
      "13\n",
      "['grocery' 'store' 'groceries' 'checkout' 'shopping' 'cashier' 'items'\n",
      " 'cart' 'shelves' 'food']\n",
      "42\n",
      "['airport' 'plane' 'flight' 'boarding' 'gate' 'airplane' 'security'\n",
      " 'departure' 'planes' 'destination']\n",
      "43\n",
      "['grocery' 'store' 'groceries' 'supermarket' 'checkout' 'shop' 'shopping'\n",
      " 'warehouse' 'cashier' 'items']\n",
      "32\n",
      "['airport' 'plane' 'flight' 'gate' 'flights' 'airplane' 'gates' 'security'\n",
      " 'departing' 'planes']\n",
      "34\n",
      "['lecture' 'class' 'classes' 'lecturing' 'classroom' 'assignment' 'hall'\n",
      " 'lesson' 'learning' 'school']\n",
      "22\n",
      "['airport' 'plane' 'flight' 'boarding' 'gate' 'airplane' 'security'\n",
      " 'passenger' 'passengers' 'concourse']\n",
      "41\n",
      "['restaurant' 'food' 'menus' 'menu' 'dining' 'meals' 'meal' 'waiter'\n",
      " 'cafe' 'sushi']\n",
      "14\n",
      "['lecture' 'class' 'lectures' 'teaching' 'classroom' 'presentation' 'hall'\n",
      " 'of' 'semester' 'professor']\n",
      "24\n",
      "['lecture' 'class' 'lectures' 'semester' 'classroom' 'assignment' 'lesson'\n",
      " 'lecturer' 'course' 'university']\n",
      "31\n",
      "['restaurant' 'food' 'menus' 'menu' 'restuarant' 'meals' 'meal' 'waiter'\n",
      " 'eating' 'hotel']\n"
     ]
    }
   ],
   "source": [
    "for key in all_correls:\n",
    "    print(key)\n",
    "    this_corr = all_correls[key]\n",
    "    this_corr = np.ravel(this_corr)\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    #print(story_index)\n",
    "    # index the top words\n",
    "    top_words = np.zeros((0,1))\n",
    "    for i in range(0, 10):\n",
    "        top_words = np.vstack((top_words, au_words[key][story_index[i]]))\n",
    "    top_words = top_words.ravel()\n",
    "    print(top_words)\n",
    "    top_ten[key]['loc'] =  top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. *Social* Top Ten\n",
    "### 1. Regress out of all the unique_wvs the loc and soc template wvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    soc = round(key/10)*10\n",
    "    # Make the inputs of the regression\n",
    "    new_input = int_stories[key].reshape(int_stories[key].shape[0],-1)\n",
    "    print(new_input.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(new_input, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, new_input.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Correlate each wv in the recall with the social wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correls = {}\n",
    "\n",
    "for key in resid_wvs:\n",
    "    corr = np.zeros((0,1))\n",
    "    soc = round(key/10)*10\n",
    "    this_soc = templates[soc].reshape(templates[soc].shape[0], -1)\n",
    "    for i in range(0, len(resid_wvs[key])):\n",
    "        this_wv = resid_wvs[key][i].reshape(resid_wvs[key][i].shape[0], -1)\n",
    "        this_correl = np.corrcoef(this_soc.T, this_wv.T)\n",
    "        # add to corr\n",
    "        corr = np.vstack((corr, this_correl[0,1]))\n",
    "    all_correls[key] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Top Ten values' indices and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[471 162 233 211 109 422 297 445 462 128]\n",
      "['breakup' 'breaking' 'for' 'break' 'what' 'reason' 'considering' 'the'\n",
      " 'relationship' 'up']\n",
      "[256 144 310 442 273 108 471 457 416  69]\n",
      "['deal' 'business' 'partnership' 'investment' 'agreement' 'success'\n",
      " 'competitor' 'deals' 'the' 'strategy']\n",
      "[376 219 248  94 258 220 375 185 280 193]\n",
      "['the' 'of' 'first' 'what' 'two' 'next' 'time' 'other' 'when' 'for']\n",
      "[395 364 381 361 320 489 393 536 462 497]\n",
      "['ring' 'diamond' 'proposal' 'proposing' 'proposed' 'diamonds' 'planning'\n",
      " 'marriage' 'the' 'engagement']\n",
      "[401 106 187 452 172 150 372 196 392  70]\n",
      "['breakup' 'breaking' 'split' 'departure' 'for' 'break' 'the' 'of'\n",
      " 'relationship' 'what']\n",
      "[474 362 339 355 294 360 487 447 452 423]\n",
      "['wedding' 'ring' 'diamond' 'proposal' 'proposed' 'planning' 'marriage'\n",
      " 'diamonds' 'engagement' 'the']\n",
      "[487 152 240 106 215 455 266 478 125 222]\n",
      "['breakup' 'breaking' 'for' 'what' 'break' 'the' 'of' 'relationship' 'up'\n",
      " 'keeping']\n",
      "[385 224 302 268 301 384 226 187 290 200]\n",
      "['the' 'of' 'interaction' 'two' 'three' 'time' 'next' 'other' 'when' 'for']\n",
      "[386 259 270  91 225 194 226 383 265 316]\n",
      "['the' 'first' 'two' 'what' 'of' 'other' 'next' 'time' 'each' 'question']\n",
      "[223 137 433 443 344 144  83 371 436 311]\n",
      "['deal' 'business' 'competitor' 'negotiating' 'position' 'disagreement'\n",
      " 'work' 'the' 'handshake' 'proposal']\n",
      "[206 125 339 390 219 393 359 207 342  58]\n",
      "['deal' 'business' 'negotiation' 'competitor' 'agreement' 'negotiating'\n",
      " 'investment' 'approach' 'financial' 'handling']\n",
      "[591 424 411 388 350 540 422 610 503 547]\n",
      "['wedding' 'ring' 'proposal' 'proposing' 'proposed' 'diamonds' 'planning'\n",
      " 'marriage' 'the' 'engagement']\n",
      "[478 316 329 124 377 277 232 275 474 383]\n",
      "['the' 'first' 'two' 'what' 'interaction' 'next' 'other' 'of' 'time'\n",
      " 'question']\n",
      "[388 122 116 181 363 210  82 379 290 166]\n",
      "['breakup' 'breaking' 'splitting' 'for' 'the' 'of' 'what' 'relationship'\n",
      " 'proposing' 'break']\n",
      "[298 271 270 238 296 371 377 385 382  36]\n",
      "['ring' 'diamond' 'proposing' 'proposed' 'planning' 'rings' 'diamonds'\n",
      " 'proposes' 'engagement' 'upcoming']\n",
      "[245 149 309 270 469 414 310 408 473 342]\n",
      "['deal' 'business' 'partnership' 'agreement' 'competitor' 'acquisition'\n",
      " 'gesture' 'the' 'handshake' 'proposal']\n"
     ]
    }
   ],
   "source": [
    "for key in all_correls:\n",
    "    this_corr = all_correls[key]\n",
    "    this_corr = np.ravel(this_corr)\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    print(story_index)\n",
    "    # index the top words\n",
    "    top_words = np.zeros((0,1))\n",
    "    for i in range(0, 10):\n",
    "        top_words = np.vstack((top_words, au_words[key][story_index[i]]))\n",
    "    top_words = top_words.ravel()\n",
    "    print(top_words)\n",
    "    top_ten[key]['soc'] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Presenting Top Ten Words\n",
    "## A. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( top_ten, open( 'top_ten3', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 11\n",
      "story\n",
      "['could' 'looked' 'might' 'hadn' 'don' 'wasn' 'hear' 'thanks' 'seeing'\n",
      " 'can']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'menu' 'diner' 'restuarant' 'waiter' 'foods'\n",
      " 'meal' 'dessert']\n",
      "soc\n",
      "['breakup' 'breaking' 'for' 'break' 'what' 'reason' 'considering' 'the'\n",
      " 'relationship' 'up']\n",
      "\n",
      "\n",
      " 12\n",
      "story\n",
      "['slightly' 'couldn' 'could' 'don' 'onto' 'ago' 'looked' 'might' 'make'\n",
      " 'can']\n",
      "loc\n",
      "['airport' 'airports' 'plane' 'flight' 'boarding' 'gate' 'gates'\n",
      " 'security' 'departure' 'arriving']\n",
      "soc\n",
      "['breakup' 'breaking' 'split' 'departure' 'for' 'break' 'the' 'of'\n",
      " 'relationship' 'what']\n",
      "\n",
      "\n",
      " 13\n",
      "story\n",
      "['wouldn' 'might' 'could' 'didn' 'doesn' 'wasn' 'nodded' 'don' 'can'\n",
      " 'looked']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'checkout' 'shopping' 'cashier' 'items'\n",
      " 'cart' 'shelves' 'food']\n",
      "soc\n",
      "['breakup' 'breaking' 'for' 'what' 'break' 'the' 'of' 'relationship' 'up'\n",
      " 'keeping']\n",
      "\n",
      "\n",
      " 14\n",
      "story\n",
      "['couldn' 'could' 'don' 'can' 'hear' 'make' 'sorry' 'doesn' 'always'\n",
      " 'shouldn']\n",
      "loc\n",
      "['lecture' 'class' 'lectures' 'teaching' 'classroom' 'presentation' 'hall'\n",
      " 'of' 'semester' 'professor']\n",
      "soc\n",
      "['breakup' 'breaking' 'splitting' 'for' 'the' 'of' 'what' 'relationship'\n",
      " 'proposing' 'break']\n",
      "\n",
      "\n",
      " 21\n",
      "story\n",
      "['could' 'nodded' 'make' 'ago' 'overlooking' 'wasn' 'can' 'into' 'gladly'\n",
      " 'down']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'menu' 'chef' 'meal' 'dishes' 'waiter'\n",
      " 'ordering' 'eating']\n",
      "soc\n",
      "['wedding' 'ring' 'diamond' 'proposal' 'proposed' 'planning' 'marriage'\n",
      " 'diamonds' 'engagement' 'the']\n",
      "\n",
      "\n",
      " 22\n",
      "story\n",
      "['couldn' 'during' 'onto' 'didn' 'don' 'hadn' 'make' 'wasn' 'throughout'\n",
      " 'velvet']\n",
      "loc\n",
      "['airport' 'plane' 'flight' 'boarding' 'gate' 'airplane' 'security'\n",
      " 'passenger' 'passengers' 'concourse']\n",
      "soc\n",
      "['wedding' 'ring' 'proposal' 'proposing' 'proposed' 'diamonds' 'planning'\n",
      " 'marriage' 'the' 'engagement']\n",
      "\n",
      "\n",
      " 23\n",
      "story\n",
      "['could' 'didn' 'looked' 'clapped' 'wasn' 'don' 'grabbed' 'got' 'seeing'\n",
      " 'pulled']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'supermarket' 'checkout' 'shop' 'shopping'\n",
      " 'checkouts' 'grocer' 'shoppers']\n",
      "soc\n",
      "['ring' 'diamond' 'proposal' 'proposing' 'proposed' 'diamonds' 'planning'\n",
      " 'marriage' 'the' 'engagement']\n",
      "\n",
      "\n",
      " 24\n",
      "story\n",
      "['during' 'towards' 'pulled' 'extremely' 'cannot' 'down' 'can' 'entering'\n",
      " 'into' 'took']\n",
      "loc\n",
      "['lecture' 'class' 'lectures' 'semester' 'classroom' 'assignment' 'lesson'\n",
      " 'lecturer' 'course' 'university']\n",
      "soc\n",
      "['ring' 'diamond' 'proposing' 'proposed' 'planning' 'rings' 'diamonds'\n",
      " 'proposes' 'engagement' 'upcoming']\n",
      "\n",
      "\n",
      " 31\n",
      "story\n",
      "['wouldn' 'could' 'don' 'somewhat' 'wooden' 'can' 'brown' 'joining'\n",
      " 'velvet' 'looks']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'menu' 'restuarant' 'meals' 'meal' 'waiter'\n",
      " 'eating' 'hotel']\n",
      "soc\n",
      "['deal' 'business' 'partnership' 'agreement' 'competitor' 'acquisition'\n",
      " 'gesture' 'the' 'handshake' 'proposal']\n",
      "\n",
      "\n",
      " 32\n",
      "story\n",
      "['onto' 'could' 'seeing' 'bottles' 'taking' 'selling' 'didn' 'letting'\n",
      " 'make' 'during']\n",
      "loc\n",
      "['airport' 'plane' 'flight' 'gate' 'flights' 'airplane' 'gates' 'security'\n",
      " 'departing' 'planes']\n",
      "soc\n",
      "['deal' 'business' 'competitor' 'negotiating' 'position' 'disagreement'\n",
      " 'work' 'the' 'handshake' 'proposal']\n",
      "\n",
      "\n",
      " 33\n",
      "story\n",
      "['doesn' 'could' 'wasn' 'isn' 'can' 'make' 'aren' 'would' 'extremely'\n",
      " 'should']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'stores' 'supermarket' 'shop' 'checkout'\n",
      " 'checkouts' 'cashier' 'purchase']\n",
      "soc\n",
      "['deal' 'business' 'partnership' 'investment' 'agreement' 'success'\n",
      " 'competitor' 'deals' 'the' 'strategy']\n",
      "\n",
      "\n",
      " 34\n",
      "story\n",
      "['could' 'don' 'can' 'during' 'cannot' 'would' 'should' 'wants'\n",
      " 'throughout' 'having']\n",
      "loc\n",
      "['lecture' 'class' 'classes' 'lecturing' 'classroom' 'assignment' 'hall'\n",
      " 'lesson' 'learning' 'school']\n",
      "soc\n",
      "['deal' 'business' 'negotiation' 'competitor' 'agreement' 'negotiating'\n",
      " 'investment' 'approach' 'financial' 'handling']\n",
      "\n",
      "\n",
      " 41\n",
      "story\n",
      "['laughed' 'greatly' 'could' 'might' 'didn' 'sweet' 'seeing' 'vegetable'\n",
      " 'looking' 'bright']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'menu' 'dining' 'meals' 'meal' 'waiter'\n",
      " 'cafe' 'sushi']\n",
      "soc\n",
      "['the' 'first' 'two' 'what' 'interaction' 'next' 'other' 'of' 'time'\n",
      " 'question']\n",
      "\n",
      "\n",
      " 42\n",
      "story\n",
      "['wouldn' 'could' 'hadn' 'might' 'slightly' 'grabbed' 'got' 'doesn'\n",
      " 'putting' 'can']\n",
      "loc\n",
      "['airport' 'plane' 'flight' 'boarding' 'gate' 'airplane' 'security'\n",
      " 'departure' 'planes' 'destination']\n",
      "soc\n",
      "['the' 'of' 'interaction' 'two' 'three' 'time' 'next' 'other' 'when' 'for']\n",
      "\n",
      "\n",
      " 43\n",
      "story\n",
      "['could' 'didn' 'isn' 'make' 'feels' 'aren' 'must' 'would' 'sounded'\n",
      " 'hear']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'supermarket' 'checkout' 'shop' 'shopping'\n",
      " 'warehouse' 'cashier' 'items']\n",
      "soc\n",
      "['the' 'first' 'two' 'what' 'of' 'other' 'next' 'time' 'each' 'question']\n",
      "\n",
      "\n",
      " 44\n",
      "story\n",
      "['couldn' 'could' 'doesn' 'don' 'wasn' 'can' 'nodded' 'knows' 'hasn'\n",
      " 'feels']\n",
      "loc\n",
      "['lecture' 'class' 'lectures' 'lecturing' 'assignment' 'hall' 'classroom'\n",
      " 'lesson' 'of' 'lecturer']\n",
      "soc\n",
      "['the' 'of' 'first' 'what' 'two' 'next' 'time' 'other' 'when' 'for']\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "\n",
    "for story in story_order:\n",
    "    print('\\n\\n',story)\n",
    "    for keys in top_ten[story]:\n",
    "        print(keys)\n",
    "        print(top_ten[story][keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11: {'story': array(['could', 'looked', 'might', 'hadn', 'don', 'wasn', 'hear',\n",
       "         'thanks', 'seeing', 'can'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'diner', 'restuarant',\n",
       "         'waiter', 'foods', 'meal', 'dessert'], dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'for', 'break', 'what', 'reason',\n",
       "         'considering', 'the', 'relationship', 'up'], dtype='<U32')},\n",
       " 33: {'story': array(['doesn', 'could', 'wasn', 'isn', 'can', 'make', 'aren', 'would',\n",
       "         'extremely', 'should'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'stores', 'supermarket', 'shop',\n",
       "         'checkout', 'checkouts', 'cashier', 'purchase'], dtype='<U32'),\n",
       "  'soc': array(['deal', 'business', 'partnership', 'investment', 'agreement',\n",
       "         'success', 'competitor', 'deals', 'the', 'strategy'], dtype='<U32')},\n",
       " 44: {'story': array(['couldn', 'could', 'doesn', 'don', 'wasn', 'can', 'nodded',\n",
       "         'knows', 'hasn', 'feels'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'lectures', 'lecturing', 'assignment', 'hall',\n",
       "         'classroom', 'lesson', 'of', 'lecturer'], dtype='<U32'),\n",
       "  'soc': array(['the', 'of', 'first', 'what', 'two', 'next', 'time', 'other',\n",
       "         'when', 'for'], dtype='<U32')},\n",
       " 23: {'story': array(['could', 'didn', 'looked', 'clapped', 'wasn', 'don', 'grabbed',\n",
       "         'got', 'seeing', 'pulled'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'supermarket', 'checkout', 'shop',\n",
       "         'shopping', 'checkouts', 'grocer', 'shoppers'], dtype='<U32'),\n",
       "  'soc': array(['ring', 'diamond', 'proposal', 'proposing', 'proposed', 'diamonds',\n",
       "         'planning', 'marriage', 'the', 'engagement'], dtype='<U32')},\n",
       " 12: {'story': array(['slightly', 'couldn', 'could', 'don', 'onto', 'ago', 'looked',\n",
       "         'might', 'make', 'can'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'airports', 'plane', 'flight', 'boarding', 'gate',\n",
       "         'gates', 'security', 'departure', 'arriving'], dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'split', 'departure', 'for', 'break', 'the',\n",
       "         'of', 'relationship', 'what'], dtype='<U32')},\n",
       " 21: {'story': array(['could', 'nodded', 'make', 'ago', 'overlooking', 'wasn', 'can',\n",
       "         'into', 'gladly', 'down'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'chef', 'meal', 'dishes',\n",
       "         'waiter', 'ordering', 'eating'], dtype='<U32'),\n",
       "  'soc': array(['wedding', 'ring', 'diamond', 'proposal', 'proposed', 'planning',\n",
       "         'marriage', 'diamonds', 'engagement', 'the'], dtype='<U32')},\n",
       " 13: {'story': array(['wouldn', 'might', 'could', 'didn', 'doesn', 'wasn', 'nodded',\n",
       "         'don', 'can', 'looked'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'checkout', 'shopping', 'cashier',\n",
       "         'items', 'cart', 'shelves', 'food'], dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'for', 'what', 'break', 'the', 'of',\n",
       "         'relationship', 'up', 'keeping'], dtype='<U32')},\n",
       " 42: {'story': array(['wouldn', 'could', 'hadn', 'might', 'slightly', 'grabbed', 'got',\n",
       "         'doesn', 'putting', 'can'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'plane', 'flight', 'boarding', 'gate', 'airplane',\n",
       "         'security', 'departure', 'planes', 'destination'], dtype='<U32'),\n",
       "  'soc': array(['the', 'of', 'interaction', 'two', 'three', 'time', 'next',\n",
       "         'other', 'when', 'for'], dtype='<U32')},\n",
       " 43: {'story': array(['could', 'didn', 'isn', 'make', 'feels', 'aren', 'must', 'would',\n",
       "         'sounded', 'hear'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'supermarket', 'checkout', 'shop',\n",
       "         'shopping', 'warehouse', 'cashier', 'items'], dtype='<U32'),\n",
       "  'soc': array(['the', 'first', 'two', 'what', 'of', 'other', 'next', 'time',\n",
       "         'each', 'question'], dtype='<U32')},\n",
       " 32: {'story': array(['onto', 'could', 'seeing', 'bottles', 'taking', 'selling', 'didn',\n",
       "         'letting', 'make', 'during'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'plane', 'flight', 'gate', 'flights', 'airplane',\n",
       "         'gates', 'security', 'departing', 'planes'], dtype='<U32'),\n",
       "  'soc': array(['deal', 'business', 'competitor', 'negotiating', 'position',\n",
       "         'disagreement', 'work', 'the', 'handshake', 'proposal'],\n",
       "        dtype='<U32')},\n",
       " 34: {'story': array(['could', 'don', 'can', 'during', 'cannot', 'would', 'should',\n",
       "         'wants', 'throughout', 'having'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'classes', 'lecturing', 'classroom',\n",
       "         'assignment', 'hall', 'lesson', 'learning', 'school'], dtype='<U32'),\n",
       "  'soc': array(['deal', 'business', 'negotiation', 'competitor', 'agreement',\n",
       "         'negotiating', 'investment', 'approach', 'financial', 'handling'],\n",
       "        dtype='<U32')},\n",
       " 22: {'story': array(['couldn', 'during', 'onto', 'didn', 'don', 'hadn', 'make', 'wasn',\n",
       "         'throughout', 'velvet'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'plane', 'flight', 'boarding', 'gate', 'airplane',\n",
       "         'security', 'passenger', 'passengers', 'concourse'], dtype='<U32'),\n",
       "  'soc': array(['wedding', 'ring', 'proposal', 'proposing', 'proposed', 'diamonds',\n",
       "         'planning', 'marriage', 'the', 'engagement'], dtype='<U32')},\n",
       " 41: {'story': array(['laughed', 'greatly', 'could', 'might', 'didn', 'sweet', 'seeing',\n",
       "         'vegetable', 'looking', 'bright'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'dining', 'meals', 'meal',\n",
       "         'waiter', 'cafe', 'sushi'], dtype='<U32'),\n",
       "  'soc': array(['the', 'first', 'two', 'what', 'interaction', 'next', 'other',\n",
       "         'of', 'time', 'question'], dtype='<U32')},\n",
       " 14: {'story': array(['couldn', 'could', 'don', 'can', 'hear', 'make', 'sorry', 'doesn',\n",
       "         'always', 'shouldn'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'lectures', 'teaching', 'classroom',\n",
       "         'presentation', 'hall', 'of', 'semester', 'professor'],\n",
       "        dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'splitting', 'for', 'the', 'of', 'what',\n",
       "         'relationship', 'proposing', 'break'], dtype='<U32')},\n",
       " 24: {'story': array(['during', 'towards', 'pulled', 'extremely', 'cannot', 'down',\n",
       "         'can', 'entering', 'into', 'took'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'lectures', 'semester', 'classroom',\n",
       "         'assignment', 'lesson', 'lecturer', 'course', 'university'],\n",
       "        dtype='<U32'),\n",
       "  'soc': array(['ring', 'diamond', 'proposing', 'proposed', 'planning', 'rings',\n",
       "         'diamonds', 'proposes', 'engagement', 'upcoming'], dtype='<U32')},\n",
       " 31: {'story': array(['wouldn', 'could', 'don', 'somewhat', 'wooden', 'can', 'brown',\n",
       "         'joining', 'velvet', 'looks'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'restuarant', 'meals',\n",
       "         'meal', 'waiter', 'eating', 'hotel'], dtype='<U32'),\n",
       "  'soc': array(['deal', 'business', 'partnership', 'agreement', 'competitor',\n",
       "         'acquisition', 'gesture', 'the', 'handshake', 'proposal'],\n",
       "        dtype='<U32')}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
