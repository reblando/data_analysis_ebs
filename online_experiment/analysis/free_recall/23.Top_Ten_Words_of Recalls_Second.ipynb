{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Top Ten Words of Recall words for each regressor\n",
    "## \"Second\" bc this is the second notebook that does this, but in this one, we regress out of the words, not only the other target regressors, but also all of the other stories and schema vectors\n",
    "### For each story there will be 3 'Top Ten Words', One for each type of regressor\n",
    "#### 1) Concatenate unique words across all participants\n",
    "#### 2) For each regressor, regress out of the words the other two vectors. \n",
    "#### 3) Correlate the residual wvs with the target regressor. \n",
    "#### 4) Index the 10 highest values, and find the 10 highest words\n",
    "#### 5) Put in dictionary\n",
    "\n",
    "\n",
    "#### 2/4/21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import copy\n",
    "from random import randrange\n",
    "from sklearn.metrics import jaccard_score\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA #for cluster analysis\n",
    "from gensim.models import KeyedVectors #for word embeddings\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "import os #for importing\n",
    "import pickle #for loading transcripts\n",
    "from scipy.stats import pearsonr \n",
    "\n",
    "# from _DRAFT_20200604_functions import * #includes constants and score function\n",
    "from tqdm import tqdm_notebook #for progress bar\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import recalls, and uncentered story and template vectors and sums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import recalls and sums\n",
    "recalls = pickle.load( open( 'fr_recalls', \"rb\" ) )\n",
    "sums = pickle.load( open( \"fr_sums\", \"rb\" ) )\n",
    "\n",
    "#import non-centered story, template\n",
    "templates = pickle.load( open( 'template_vectors', \"rb\" ) )\n",
    "stories = pickle.load( open( 'actual_story_vectors', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipath = 'rolando/wiki-news-300d-1M.vec'\n",
    "wv_model = KeyedVectors.load_word2vec_format(wikipath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = 300\n",
    "\n",
    "# FastText preprocessing, based on bittlingmayer/ft_wiki_preproc.py\n",
    "# Remove special characters, put spaces between all tokens\n",
    "SUB = [\"s/’/'/g\", \"s/′/'/g\", \"s/''/ /g\", \"s/'/ ' /g\", 's/“/\"/g', 's/”/\"/g', 's/\"/ /g', \"s/\\\\./ \\\\. /g\", \"s/<br \\\\/>/ /g\", \"s/, / , /g\", \"s/(/ ( /g\", \"s/)/ ) /g\", \"s/\\\\!/ \\\\! /g\", \"s/\\\\?/ \\\\? /g\", \"s/\\\\;/ /g\", \"s/\\\\:/ /g\", \"s/-/ - /g\", \"s/=/ /g\", \"s/=/ /g\", \"s/*/ /g\", \"s/|/ /g\", \"s/«/ /g\", \n",
    "       \"s/…/ /g\", \"s/‘/ /g\", \"s/í/ /g\", \"s/ñ/ /g\", \"s/\\x84/ /g\", \"s/î/ /g\", \"s/ó/ /g\", \"s/\\x83/ /g\", \"s/ï/ /g\", \"s/õ/ /g\",\n",
    "       \"s/ò/ /g\", \"s/,/ /g\", \"s/ô/ /g\", \"s/\\x92/ /g\", \"s/é/ /g\", \"s/\\x8e/ /g\", \"s/â\\x80¦/ /g\", \"s/\\x91/ /g\", \"s/\\x93/ /g\",\n",
    "       \"s/\\x94/ /g\", \"s/ã®/ /g\", \"s/ã¨/ /g\", \"s/ã©/ /g\",\n",
    "       \"s/\\â\\x80\\x99/ /g\", \"s/â\\x80\\x9c/ /g\", \"s/â\\x80\\x9d/ /g\", \"s/â\\x80\\x99/ /g\", \"s/â\\x80\\x9c/ /g\", \"s/â\\x80\\x98/ /g\",\n",
    "       \"s/â/ /g\"]\n",
    "\n",
    "def __normalize_text(s):\n",
    "    for sg in SUB:\n",
    "        rep = sg.replace('\\\\','').split('/')\n",
    "        s = s.replace(rep[1], rep[2])\n",
    "    s = s.replace('/',' ')\n",
    "    return s\n",
    "\n",
    "def __spaces(s):\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def __digits(s):\n",
    "    return ''.join(filter(lambda c: not c.isdigit(), s))\n",
    "\n",
    "# def preproc(s):\n",
    "#     return __punctuation(__spaces(__digits(__normalize_text(s.lower()))))\n",
    "\n",
    "def preproc(s):\n",
    "    return (__spaces(__digits(__normalize_text(s.lower()))))\n",
    "\n",
    "def __punctuation(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def word2vecSent(sentence, model = 'fasttext'):\n",
    "    wv_dim = 300 #for glove and fasttext\n",
    "    \n",
    "    if model == 'glove':\n",
    "        wvmodel = glove_model\n",
    "    elif model == 'fasttext':\n",
    "        wvmodel = wv_model\n",
    "        \n",
    "    words = preproc(sentence).split(' ')\n",
    "    wv = np.zeros((len(words), wv_dim))\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in wvmodel.vocab:\n",
    "            wv[i,:] = wvmodel.word_vec(words[i])\n",
    "    \n",
    "    return words, wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. 3 sets of top ten \n",
    "## A. Find unique words in recalls of each story\n",
    "### 1. Concatenate words and word vectors in parallel across all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = {}\n",
    "all_wvs = {}\n",
    "\n",
    "#iterate through all recalls in a story and concatente words and wvs of all stories\n",
    "for key in recalls:\n",
    "    words = np.zeros((0,1))\n",
    "    wvs = np.zeros((0,300))\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,len(recalls[key][i])):\n",
    "            p_words, p_wvs = word2vecSent(recalls[key][i][j][0])\n",
    "            # reshape p_words\n",
    "            p_words = np.array(p_words)\n",
    "            p_words = p_words.reshape(p_words.shape[0],-1)\n",
    "            # stack p_words and p_wvs\n",
    "            words = np.vstack((words, p_words))\n",
    "            wvs = np.vstack((wvs, p_wvs))\n",
    "    all_words[key] = words\n",
    "    all_wvs[key] = wvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Centering all words + story wvs + template wvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all words from recall\n",
    "\n",
    "concat = np.zeros((0,300))\n",
    "\n",
    "for key in all_wvs:\n",
    "    for i in range(0, len(all_wvs[key])):\n",
    "        concat = np.vstack((concat, all_wvs[key][i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37545, 300)\n"
     ]
    }
   ],
   "source": [
    "print(concat.shape)\n",
    "for key in stories:\n",
    "    concat = np.vstack((concat, stories[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37561, 300)\n",
      "(37569, 300)\n"
     ]
    }
   ],
   "source": [
    "print(concat.shape)\n",
    "for key in templates:\n",
    "    concat = np.vstack((concat, templates[key]))\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "centering_vec = np.mean(concat, axis = 0)\n",
    "\n",
    "# template vectors\n",
    "\n",
    "for key in templates:\n",
    "    templates[key] = templates[key] - centering_vec\n",
    "    \n",
    "# recall vectors\n",
    "\n",
    "for key in recalls:\n",
    "    for i in range(0, len(all_wvs[key])):\n",
    "        all_wvs[key][i] = all_wvs[key][i] - centering_vec\n",
    "            \n",
    "#story vectors\n",
    "\n",
    "#make new dict with int key instead of string\n",
    "int_stories = {}\n",
    "for key in stories:\n",
    "    int_stories[int(key)] = stories[key] - centering_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find unique words and wvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_words = {}\n",
    "au_wvs = {}\n",
    "\n",
    "for key in all_words:\n",
    "    # find unique wvs and their indices:\n",
    "    unique_wvs, index = np.unique(all_wvs[key], axis=0, return_index=True)\n",
    "    unique_words =  all_words[key][index]\n",
    "    # put in dicts\n",
    "    au_wvs[key] = unique_wvs\n",
    "    au_words[key] = unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. *Story* Top Ten\n",
    "### A. Regress out of all the unique_wvs the loc and soc template wvs\n",
    "#### 1. Concatenating the schema part of the matrix used as input to regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_order = [1, 2, 3, 4, 10, 20, 30, 40]\n",
    "\n",
    "this_input2 = np.zeros((300,0))\n",
    "\n",
    "for i in schema_order:\n",
    "    this_input2 = np.concatenate((this_input2, templates[i].reshape(templates[i].shape[0],-1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    # add on all other stories to the regression input\n",
    "    new_input = np.copy(this_input2)\n",
    "    for story in story_order:\n",
    "        if story == key:\n",
    "            print('skip')\n",
    "        else:\n",
    "            new_input = np.concatenate((new_input, stories[str(story)].reshape(stories[str(story)].shape[0],-1)), axis = 1)\n",
    "    print(new_input.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(new_input, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, new_input.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Correlate each wv in the recall with the story wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correls = {}\n",
    "\n",
    "for key in resid_wvs:\n",
    "    corr = np.zeros((0,1))\n",
    "    this_story = stories[str(key)].reshape(stories[str(key)].shape[0], -1)\n",
    "    for i in range(0, len(resid_wvs[key])):\n",
    "        this_wv = resid_wvs[key][i].reshape(resid_wvs[key][i].shape[0], -1)\n",
    "        this_correl = np.corrcoef(this_story.T, this_wv.T)\n",
    "        # add to corr\n",
    "        corr = np.vstack((corr, this_correl[0,1]))\n",
    "    all_correls[key] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Top Ten values' indices and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['thomas' 'sat' 'got' 't' \"'\" 'hang' 'burguer' 'hadn' 'ate' 'bar']\n",
      "33\n",
      "['ms' 'cheeses' 'stores' 'cheese' 'product' 'products' 'morrison' '.'\n",
      " 'crackers' 'marketed']\n",
      "44\n",
      "['found' 'egyptology' 'mummification' 'saw' 'wrote' 'lived' 'seen' 'knew'\n",
      " 'during' 'looked']\n",
      "23\n",
      "['alex' 'chloe' 'excitement' 'loves' 'likes' 'crowd' 'day' 'everyone'\n",
      " 'friends' 'mom']\n",
      "12\n",
      "['calvin' 'ultimately' 'they' 'conveyor' 'not' 'properly' 'belt' 'was'\n",
      " 'delay' 'were']\n",
      "21\n",
      "['senna' 'oysters' 'dishes' 'caramelized' 'champagne' 'chandeliers'\n",
      " 'delicious' 'scallops' 'garlic' 'tears']\n",
      "13\n",
      "['tonight' 'going' 'think' 'well' 'agreeing' 'trying' 'said' 'however'\n",
      " 'nodded' 'anyway']\n",
      "42\n",
      "['she' 'cart' 'might' 'had' 'her' 'was' 'gave' 'could' 'anna' 'of']\n",
      "43\n",
      "['carina' 'simon' 'ribs' 'meat' 'thought' 'chickens' 'blushed' 'met'\n",
      " 'considered' 'headed']\n",
      "32\n",
      "['jeff' 'william' 'wine' 'steep' 'd' '$' 'vineyard' 'per' 'wines'\n",
      " 'vineyards']\n",
      "34\n",
      "['alma' 'fund' 'funds' 'funding' 'investors' 'project' 'investor'\n",
      " 'finance' 'capital' 'investment']\n",
      "22\n",
      "['belongings' 'digging' 'sven' 'luggage' 'their' 'away' 'duffle' 'bag'\n",
      " 'onto' 'shoes']\n",
      "41\n",
      "['teresa' 'james' 'theresa' 'jessica' 'waitress' 'her' 'john' 'she' 'him'\n",
      " 'english']\n",
      "14\n",
      "['i' 'maria' 'my' 'her' 'henry' 'me' 'you' 'think' 'she' 'if']\n",
      "24\n",
      "['aaron' 'lana' 'protons' 'sodium' 'nucleus' 'classmate' 'computer' 'ions'\n",
      " 'number' 'sheet']\n",
      "31\n",
      "['mr' '.' 'gordon' 'large' 'his' 'small' 'smaller' 'business' 'larger'\n",
      " 'company']\n"
     ]
    }
   ],
   "source": [
    "for key in all_correls:\n",
    "    print(key)\n",
    "    this_corr = all_correls[key]\n",
    "    this_corr = np.ravel(this_corr)\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    #print(story_index)\n",
    "    # index the top words\n",
    "    top_words = np.zeros((0,1))\n",
    "    for i in range(0, 10):\n",
    "        top_words = np.vstack((top_words, au_words[key][story_index[i]]))\n",
    "    top_words = top_words.ravel()\n",
    "    print(top_words)\n",
    "    top_ten[key] = {'story': top_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. *Location* Top Ten\n",
    "### 1. Regress out of all the unique_wvs the story and soc template wvs\n",
    "#### a. Concatenating the story part of the matrix used as input to regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 16)\n"
     ]
    }
   ],
   "source": [
    "this_input = np.zeros((300,0))\n",
    "\n",
    "for story in story_order:\n",
    "    this_input = np.concatenate((this_input, stories[str(story)].reshape(stories[str(story)].shape[0],-1)), axis = 1)\n",
    "\n",
    "print(this_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n"
     ]
    }
   ],
   "source": [
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    loc = key%10\n",
    "    # Make the inputs of the regression\n",
    "    new_input = np.copy(this_input)\n",
    "    for schema in schema_order:\n",
    "        if schema == loc:\n",
    "            print('skip')\n",
    "        else:\n",
    "            new_input = np.concatenate((new_input, templates[schema].reshape(templates[schema].shape[0],-1)), axis = 1)\n",
    "    print(new_input.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(new_input, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, new_input.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Correlate each wv in the recall with the story wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correls = {}\n",
    "\n",
    "for key in resid_wvs:\n",
    "    corr = np.zeros((0,1))\n",
    "    loc = key%10\n",
    "    this_loc = templates[loc].reshape(templates[loc].shape[0], -1)\n",
    "    for i in range(0, len(resid_wvs[key])):\n",
    "        this_wv = resid_wvs[key][i].reshape(resid_wvs[key][i].shape[0], -1)\n",
    "        this_correl = np.corrcoef(this_loc.T, this_wv.T)\n",
    "        # add to corr\n",
    "        corr = np.vstack((corr, this_correl[0,1]))\n",
    "    all_correls[key] = corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top Ten values' indices and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['restaurant' 'food' 'menus' 'restuarant' 'foods' 'menu' 'meal' 'eating'\n",
      " 'diner' 'restauraunt']\n",
      "33\n",
      "['grocery' 'store' 'groceries' 'checkout' 'stores' 'shop' 'supermarket'\n",
      " 'cashier' 'checkouts' 'wholesale']\n",
      "44\n",
      "['lecture' 'class' 'lecturing' 'lectures' 'hall' 'classroom' 'lecturer'\n",
      " 'attending' 'interrupting' 'school']\n",
      "23\n",
      "['grocery' 'store' 'groceries' 'checkout' 'shop' 'grocer' 'supermarket'\n",
      " 'cashier' 'checkouts' 'buy']\n",
      "12\n",
      "['airport' 'airports' 'plane' 'boarding' 'gate' 'flight' 'gates' 'flying'\n",
      " 'terminal' 'arriving']\n",
      "21\n",
      "['restaurant' 'food' 'menus' 'chef' 'menu' 'meal' 'ordering' 'eating'\n",
      " 'dishes' 'patrons']\n",
      "13\n",
      "['grocery' 'store' 'groceries' 'checkout' 'cashier' 'cart' 'buy' 'shelves'\n",
      " 'shelf' 'shopping']\n",
      "42\n",
      "['airport' 'plane' 'planes' 'boarding' 'airplane' 'gate' 'flight'\n",
      " 'terminal' 'security' 'at']\n",
      "43\n",
      "['grocery' 'store' 'groceries' 'checkout' 'shop' 'supermarket' 'cashier'\n",
      " 'pick' 'shopping' 'counter']\n",
      "32\n",
      "['airport' 'plane' 'planes' 'flights' 'airplane' 'gate' 'flight'\n",
      " 'destinations' 'gates' 'departing']\n",
      "34\n",
      "['lecture' 'class' 'lecturing' 'classes' 'hall' 'classroom' 'packing'\n",
      " 'doing' 'learning' 'school']\n",
      "22\n",
      "['airport' 'plane' 'tarmac' 'boarding' 'airplane' 'gate' 'flight'\n",
      " 'passengers' 'concourse' 'flying']\n",
      "41\n",
      "['restaurant' 'food' 'menus' 'meals' 'menu' 'meal' 'dining' 'sushi'\n",
      " 'ordered' 'order']\n",
      "14\n",
      "['lecture' 'class' 'lectures' 'teaching' 'hall' 'classroom' 'taught'\n",
      " 'lecturer' 'semester' 'doing']\n",
      "24\n",
      "['lecture' 'class' 'lectures' 'hall' 'classroom' 'taught' 'lecturer'\n",
      " 'semester' 'course' 'what']\n",
      "31\n",
      "['restaurant' 'food' 'menus' 'restuarant' 'meals' 'menu' 'meal' 'ordering'\n",
      " 'eating' 'ordered']\n"
     ]
    }
   ],
   "source": [
    "for key in all_correls:\n",
    "    print(key)\n",
    "    this_corr = all_correls[key]\n",
    "    this_corr = np.ravel(this_corr)\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    #print(story_index)\n",
    "    # index the top words\n",
    "    top_words = np.zeros((0,1))\n",
    "    for i in range(0, 10):\n",
    "        top_words = np.vstack((top_words, au_words[key][story_index[i]]))\n",
    "    top_words = top_words.ravel()\n",
    "    print(top_words)\n",
    "    top_ten[key]['loc'] =  top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. *Social* Top Ten\n",
    "### 1. Regress out of all the unique_wvs the loc and soc template wvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n",
      "skip\n",
      "(300, 23)\n"
     ]
    }
   ],
   "source": [
    "resid_wvs = {}\n",
    "\n",
    "for key in au_wvs:\n",
    "    these_residuals = np.zeros((0, 300))\n",
    "    soc = round(key/10)*10\n",
    "    # Make the inputs of the regression\n",
    "    new_input = np.copy(this_input)\n",
    "    for schema in schema_order:\n",
    "        if schema == soc:\n",
    "            print('skip')\n",
    "        else:\n",
    "            new_input = np.concatenate((new_input, templates[schema].reshape(templates[schema].shape[0],-1)), axis = 1)\n",
    "    print(new_input.shape)\n",
    "    for i in range(0,len(au_wvs[key])):\n",
    "        output = np.copy(au_wvs[key][i])\n",
    "        #output = output.reshape(output.shape[0],-1)\n",
    "        model = LinearRegression().fit(new_input, output)\n",
    "        ## Is subtracting the intercept correct?? \n",
    "        new_vec = output - np.dot(model.coef_, new_input.T)- model.intercept_\n",
    "        these_residuals = np.vstack((these_residuals, new_vec))\n",
    "    resid_wvs[key] = these_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Correlate each wv in the recall with the social wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correls = {}\n",
    "\n",
    "for key in resid_wvs:\n",
    "    corr = np.zeros((0,1))\n",
    "    soc = round(key/10)*10\n",
    "    this_soc = templates[soc].reshape(templates[soc].shape[0], -1)\n",
    "    for i in range(0, len(resid_wvs[key])):\n",
    "        this_wv = resid_wvs[key][i].reshape(resid_wvs[key][i].shape[0], -1)\n",
    "        this_correl = np.corrcoef(this_soc.T, this_wv.T)\n",
    "        # add to corr\n",
    "        corr = np.vstack((corr, this_correl[0,1]))\n",
    "    all_correls[key] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Top Ten values' indices and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[471 162 128 211 393 430 233 307  58 297]\n",
      "['breakup' 'breaking' 'up' 'break' 'breaks' 'wanting' 'for' 'ending'\n",
      " 'follow' 'considering']\n",
      "[256 144 457 310 273 471 472  83 442 347]\n",
      "['deal' 'business' 'deals' 'partnership' 'agreement' 'competitor'\n",
      " 'handshake' 'work' 'investment' 'profit']\n",
      "[376 258 220 185 280 248 241 252  94 223]\n",
      "['the' 'two' 'next' 'other' 'when' 'first' 'couple' 'each' 'what' 'all']\n",
      "[395 364 489 320 361 381 439 393 504 509]\n",
      "['ring' 'diamond' 'diamonds' 'proposed' 'proposing' 'proposal' 'been'\n",
      " 'planning' 'proposes' 'fiance']\n",
      "[401 106  88 187 150 418 232 452 358 146]\n",
      "['breakup' 'breaking' 'up' 'split' 'break' 'stated' 'citing' 'departure'\n",
      " 'wanting' 'expressed']\n",
      "[362 339 474 447 294 355 394 360 455 461]\n",
      "['ring' 'diamond' 'wedding' 'diamonds' 'proposed' 'proposal' 'been'\n",
      " 'planning' 'proposes' 'fiancee']\n",
      "[487 152 125 319 428 215 403 443 222 496]\n",
      "['breakup' 'breaking' 'up' 'broke' 'broken' 'break' 'breaks' 'wanting'\n",
      " 'keeping' 'led']\n",
      "[385 268 226 187 290 301 259 302 298 350]\n",
      "['the' 'two' 'next' 'other' 'when' 'three' 'each' 'interaction' 'once'\n",
      " 'later']\n",
      "[386 270 226 194 293 259 253 265  91 229]\n",
      "['the' 'two' 'next' 'other' 'when' 'first' 'couple' 'each' 'what' 'all']\n",
      "[223 137 126 433 436  83 413 344  77 391]\n",
      "['deal' 'business' 'offer' 'competitor' 'handshake' 'work' 'money'\n",
      " 'position' 'complaint' 'made']\n",
      "[206 125 109 219 390  75 359 376 291 339]\n",
      "['deal' 'business' 'offer' 'agreement' 'competitor' 'work' 'investment'\n",
      " 'money' 'profit' 'negotiation']\n",
      "[424 591 540 350 388 411 470 422 555 545]\n",
      "['ring' 'wedding' 'diamonds' 'proposed' 'proposing' 'proposal' 'been'\n",
      " 'planning' 'proposes' 'originally']\n",
      "[478 329 277 232 360 316 309 322 377 124]\n",
      "['the' 'two' 'next' 'other' 'when' 'first' 'couple' 'each' 'interaction'\n",
      " 'what']\n",
      "[388 122 116 103 335 166 396 181 251 398]\n",
      "['breakup' 'breaking' 'splitting' 'up' 'broken' 'break' 'stated' 'for'\n",
      " 'ending' 'dating']\n",
      "[298 271 371 377 238 270 332 296 385 302]\n",
      "['ring' 'diamond' 'rings' 'diamonds' 'proposed' 'proposing' 'been'\n",
      " 'planning' 'proposes' 'has']\n",
      "[245 149 310 134 309 270 469 473 453 488]\n",
      "['deal' 'business' 'gesture' 'offer' 'partnership' 'agreement'\n",
      " 'competitor' 'handshake' 'money' 'assurances']\n"
     ]
    }
   ],
   "source": [
    "for key in all_correls:\n",
    "    this_corr = all_correls[key]\n",
    "    this_corr = np.ravel(this_corr)\n",
    "    story_index = this_corr.argsort()[-10:][::-1]\n",
    "    print(story_index)\n",
    "    # index the top words\n",
    "    top_words = np.zeros((0,1))\n",
    "    for i in range(0, 10):\n",
    "        top_words = np.vstack((top_words, au_words[key][story_index[i]]))\n",
    "    top_words = top_words.ravel()\n",
    "    print(top_words)\n",
    "    top_ten[key]['soc'] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Presenting Top Ten Words\n",
    "## A. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( top_ten, open( 'top_ten2', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 11\n",
      "story\n",
      "['thomas' 'sat' 'got' 't' \"'\" 'hang' 'burguer' 'hadn' 'ate' 'bar']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'restuarant' 'foods' 'menu' 'meal' 'eating'\n",
      " 'diner' 'restauraunt']\n",
      "soc\n",
      "['breakup' 'breaking' 'up' 'break' 'breaks' 'wanting' 'for' 'ending'\n",
      " 'follow' 'considering']\n",
      "\n",
      "\n",
      " 12\n",
      "story\n",
      "['calvin' 'ultimately' 'they' 'conveyor' 'not' 'properly' 'belt' 'was'\n",
      " 'delay' 'were']\n",
      "loc\n",
      "['airport' 'airports' 'plane' 'boarding' 'gate' 'flight' 'gates' 'flying'\n",
      " 'terminal' 'arriving']\n",
      "soc\n",
      "['breakup' 'breaking' 'up' 'split' 'break' 'stated' 'citing' 'departure'\n",
      " 'wanting' 'expressed']\n",
      "\n",
      "\n",
      " 13\n",
      "story\n",
      "['tonight' 'going' 'think' 'well' 'agreeing' 'trying' 'said' 'however'\n",
      " 'nodded' 'anyway']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'checkout' 'cashier' 'cart' 'buy' 'shelves'\n",
      " 'shelf' 'shopping']\n",
      "soc\n",
      "['breakup' 'breaking' 'up' 'broke' 'broken' 'break' 'breaks' 'wanting'\n",
      " 'keeping' 'led']\n",
      "\n",
      "\n",
      " 14\n",
      "story\n",
      "['i' 'maria' 'my' 'her' 'henry' 'me' 'you' 'think' 'she' 'if']\n",
      "loc\n",
      "['lecture' 'class' 'lectures' 'teaching' 'hall' 'classroom' 'taught'\n",
      " 'lecturer' 'semester' 'doing']\n",
      "soc\n",
      "['breakup' 'breaking' 'splitting' 'up' 'broken' 'break' 'stated' 'for'\n",
      " 'ending' 'dating']\n",
      "\n",
      "\n",
      " 21\n",
      "story\n",
      "['senna' 'oysters' 'dishes' 'caramelized' 'champagne' 'chandeliers'\n",
      " 'delicious' 'scallops' 'garlic' 'tears']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'chef' 'menu' 'meal' 'ordering' 'eating'\n",
      " 'dishes' 'patrons']\n",
      "soc\n",
      "['ring' 'diamond' 'wedding' 'diamonds' 'proposed' 'proposal' 'been'\n",
      " 'planning' 'proposes' 'fiancee']\n",
      "\n",
      "\n",
      " 22\n",
      "story\n",
      "['belongings' 'digging' 'sven' 'luggage' 'their' 'away' 'duffle' 'bag'\n",
      " 'onto' 'shoes']\n",
      "loc\n",
      "['airport' 'plane' 'tarmac' 'boarding' 'airplane' 'gate' 'flight'\n",
      " 'passengers' 'concourse' 'flying']\n",
      "soc\n",
      "['ring' 'wedding' 'diamonds' 'proposed' 'proposing' 'proposal' 'been'\n",
      " 'planning' 'proposes' 'originally']\n",
      "\n",
      "\n",
      " 23\n",
      "story\n",
      "['alex' 'chloe' 'excitement' 'loves' 'likes' 'crowd' 'day' 'everyone'\n",
      " 'friends' 'mom']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'checkout' 'shop' 'grocer' 'supermarket'\n",
      " 'cashier' 'checkouts' 'buy']\n",
      "soc\n",
      "['ring' 'diamond' 'diamonds' 'proposed' 'proposing' 'proposal' 'been'\n",
      " 'planning' 'proposes' 'fiance']\n",
      "\n",
      "\n",
      " 24\n",
      "story\n",
      "['aaron' 'lana' 'protons' 'sodium' 'nucleus' 'classmate' 'computer' 'ions'\n",
      " 'number' 'sheet']\n",
      "loc\n",
      "['lecture' 'class' 'lectures' 'hall' 'classroom' 'taught' 'lecturer'\n",
      " 'semester' 'course' 'what']\n",
      "soc\n",
      "['ring' 'diamond' 'rings' 'diamonds' 'proposed' 'proposing' 'been'\n",
      " 'planning' 'proposes' 'has']\n",
      "\n",
      "\n",
      " 31\n",
      "story\n",
      "['mr' '.' 'gordon' 'large' 'his' 'small' 'smaller' 'business' 'larger'\n",
      " 'company']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'restuarant' 'meals' 'menu' 'meal' 'ordering'\n",
      " 'eating' 'ordered']\n",
      "soc\n",
      "['deal' 'business' 'gesture' 'offer' 'partnership' 'agreement'\n",
      " 'competitor' 'handshake' 'money' 'assurances']\n",
      "\n",
      "\n",
      " 32\n",
      "story\n",
      "['jeff' 'william' 'wine' 'steep' 'd' '$' 'vineyard' 'per' 'wines'\n",
      " 'vineyards']\n",
      "loc\n",
      "['airport' 'plane' 'planes' 'flights' 'airplane' 'gate' 'flight'\n",
      " 'destinations' 'gates' 'departing']\n",
      "soc\n",
      "['deal' 'business' 'offer' 'competitor' 'handshake' 'work' 'money'\n",
      " 'position' 'complaint' 'made']\n",
      "\n",
      "\n",
      " 33\n",
      "story\n",
      "['ms' 'cheeses' 'stores' 'cheese' 'product' 'products' 'morrison' '.'\n",
      " 'crackers' 'marketed']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'checkout' 'stores' 'shop' 'supermarket'\n",
      " 'cashier' 'checkouts' 'wholesale']\n",
      "soc\n",
      "['deal' 'business' 'deals' 'partnership' 'agreement' 'competitor'\n",
      " 'handshake' 'work' 'investment' 'profit']\n",
      "\n",
      "\n",
      " 34\n",
      "story\n",
      "['alma' 'fund' 'funds' 'funding' 'investors' 'project' 'investor'\n",
      " 'finance' 'capital' 'investment']\n",
      "loc\n",
      "['lecture' 'class' 'lecturing' 'classes' 'hall' 'classroom' 'packing'\n",
      " 'doing' 'learning' 'school']\n",
      "soc\n",
      "['deal' 'business' 'offer' 'agreement' 'competitor' 'work' 'investment'\n",
      " 'money' 'profit' 'negotiation']\n",
      "\n",
      "\n",
      " 41\n",
      "story\n",
      "['teresa' 'james' 'theresa' 'jessica' 'waitress' 'her' 'john' 'she' 'him'\n",
      " 'english']\n",
      "loc\n",
      "['restaurant' 'food' 'menus' 'meals' 'menu' 'meal' 'dining' 'sushi'\n",
      " 'ordered' 'order']\n",
      "soc\n",
      "['the' 'two' 'next' 'other' 'when' 'first' 'couple' 'each' 'interaction'\n",
      " 'what']\n",
      "\n",
      "\n",
      " 42\n",
      "story\n",
      "['she' 'cart' 'might' 'had' 'her' 'was' 'gave' 'could' 'anna' 'of']\n",
      "loc\n",
      "['airport' 'plane' 'planes' 'boarding' 'airplane' 'gate' 'flight'\n",
      " 'terminal' 'security' 'at']\n",
      "soc\n",
      "['the' 'two' 'next' 'other' 'when' 'three' 'each' 'interaction' 'once'\n",
      " 'later']\n",
      "\n",
      "\n",
      " 43\n",
      "story\n",
      "['carina' 'simon' 'ribs' 'meat' 'thought' 'chickens' 'blushed' 'met'\n",
      " 'considered' 'headed']\n",
      "loc\n",
      "['grocery' 'store' 'groceries' 'checkout' 'shop' 'supermarket' 'cashier'\n",
      " 'pick' 'shopping' 'counter']\n",
      "soc\n",
      "['the' 'two' 'next' 'other' 'when' 'first' 'couple' 'each' 'what' 'all']\n",
      "\n",
      "\n",
      " 44\n",
      "story\n",
      "['found' 'egyptology' 'mummification' 'saw' 'wrote' 'lived' 'seen' 'knew'\n",
      " 'during' 'looked']\n",
      "loc\n",
      "['lecture' 'class' 'lecturing' 'lectures' 'hall' 'classroom' 'lecturer'\n",
      " 'attending' 'interrupting' 'school']\n",
      "soc\n",
      "['the' 'two' 'next' 'other' 'when' 'first' 'couple' 'each' 'what' 'all']\n"
     ]
    }
   ],
   "source": [
    "story_order = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "\n",
    "for story in story_order:\n",
    "    print('\\n\\n',story)\n",
    "    for keys in top_ten[story]:\n",
    "        print(keys)\n",
    "        print(top_ten[story][keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11: {'story': array(['hadn', 'could', 'might', 'wasn', 'don', 'can', 'doesn', 'hear',\n",
       "         'got', 'make'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'diner', 'restuarant',\n",
       "         'foods', 'meal', 'waiter', 'dessert'], dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'for', 'of', 'the', 'break', 'relationship',\n",
       "         'what', 'considering', 'ending'], dtype='<U32')},\n",
       " 33: {'story': array(['could', 'doesn', 'wasn', 'can', 'isn', 'should', 'would', 'make',\n",
       "         'extremely', 'knows'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'stores', 'supermarket',\n",
       "         'checkout', 'shop', 'checkouts', 'purchase', 'cashier'],\n",
       "        dtype='<U32'),\n",
       "  'soc': array(['deal', 'partnership', 'business', 'agreement', 'investment',\n",
       "         'success', 'the', 'handshake', 'deals', 'competitor'], dtype='<U32')},\n",
       " 44: {'story': array(['couldn', 'could', 'wasn', 'can', 'doesn', 'hasn', 'don', 'seeing',\n",
       "         'looked', 'nodded'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'lectures', 'lecturing', 'classroom',\n",
       "         'assignment', 'hall', 'lesson', 'professor', 'lecturer'],\n",
       "        dtype='<U32'),\n",
       "  'soc': array(['the', 'two', 'first', 'other', 'what', 'couple', 'of', 'time',\n",
       "         'when', 'next'], dtype='<U32')},\n",
       " 23: {'story': array(['could', 'didn', 'wasn', 'don', 'got', 'seeing', 'looked',\n",
       "         'cannot', 'knows', 'would'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'supermarket', 'checkout', 'shop',\n",
       "         'shopping', 'checkouts', 'grocer', 'shoppers'], dtype='<U32'),\n",
       "  'soc': array(['ring', 'diamond', 'proposed', 'proposing', 'proposal', 'marriage',\n",
       "         'diamonds', 'the', 'planning', 'engagement'], dtype='<U32')},\n",
       " 12: {'story': array(['couldn', 'could', 'might', 'didn', 'can', 'don', 'doesn',\n",
       "         'slightly', 'ago', 'should'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'airports', 'plane', 'flight', 'boarding', 'gate',\n",
       "         'gates', 'security', 'departure', 'terminal'], dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'split', 'for', 'of', 'what', 'the',\n",
       "         'break', 'relationship', 'departure'], dtype='<U32')},\n",
       " 21: {'story': array(['could', 'nodded', 'wasn', 'ago', 'can', 'make', 'always',\n",
       "         'gladly', 'looked', 'got'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'chef', 'dishes', 'meal',\n",
       "         'waiter', 'dessert', 'eating'], dtype='<U32'),\n",
       "  'soc': array(['ring', 'wedding', 'diamond', 'proposed', 'proposal', 'marriage',\n",
       "         'diamonds', 'the', 'planning', 'engagement'], dtype='<U32')},\n",
       " 13: {'story': array(['wouldn', 'could', 'might', 'wasn', 'didn', 'don', 'can', 'doesn',\n",
       "         'always', 'nodded'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'checkout', 'shopping', 'cashier',\n",
       "         'items', 'cart', 'food', 'products'], dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'for', 'the', 'of', 'relationship', 'break',\n",
       "         'what', 'with', 'by'], dtype='<U32')},\n",
       " 42: {'story': array(['could', 'wouldn', 'might', 'hadn', 'slightly', 'can', 'doesn',\n",
       "         'got', 'shouldn', 'make'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'plane', 'flight', 'boarding', 'gate', 'airplane',\n",
       "         'security', 'departure', 'planes', 'destination'], dtype='<U32'),\n",
       "  'soc': array(['the', 'two', 'of', 'interaction', 'three', 'other', 'next',\n",
       "         'when', 'whether', 'for'], dtype='<U32')},\n",
       " 43: {'story': array(['could', 'didn', 'sounded', 'got', 'feels', 'looked', 'looks',\n",
       "         'seeing', 'hear', 'would'], dtype='<U32'),\n",
       "  'loc': array(['grocery', 'store', 'groceries', 'supermarket', 'checkout', 'shop',\n",
       "         'shopping', 'warehouse', 'cashier', 'items'], dtype='<U32'),\n",
       "  'soc': array(['the', 'of', 'first', 'what', 'two', 'next', 'other', 'time',\n",
       "         'when', 'for'], dtype='<U32')},\n",
       " 32: {'story': array(['could', 'didn', 'wasn', 'seeing', 'make', 'would', 'isn', 'got',\n",
       "         'amount', 'want'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'plane', 'flight', 'gate', 'gates', 'flights',\n",
       "         'airplane', 'security', 'planes', 'departing'], dtype='<U32'),\n",
       "  'soc': array(['deal', 'business', 'competitor', 'disagreement', 'proposal',\n",
       "         'negotiating', 'initial', 'work', 'amount', 'position'],\n",
       "        dtype='<U32')},\n",
       " 34: {'story': array(['could', 'don', 'can', 'cannot', 'should', 'would', 'during',\n",
       "         'wants', 'amount', 'have'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'classes', 'lecturing', 'classroom',\n",
       "         'assignment', 'hall', 'lesson', 'professor', 'of'], dtype='<U32'),\n",
       "  'soc': array(['deal', 'business', 'agreement', 'investment', 'competitor',\n",
       "         'negotiation', 'financial', 'negotiating', 'initial', 'offer'],\n",
       "        dtype='<U32')},\n",
       " 22: {'story': array(['couldn', 'didn', 'hadn', 'wasn', 'onto', 'got', 'don', 'looked',\n",
       "         'make', 'hear'], dtype='<U32'),\n",
       "  'loc': array(['airport', 'plane', 'flight', 'boarding', 'gate', 'airplane',\n",
       "         'security', 'passenger', 'passengers', 'baggage'], dtype='<U32'),\n",
       "  'soc': array(['wedding', 'ring', 'proposed', 'proposing', 'proposal', 'diamonds',\n",
       "         'marriage', 'proposes', 'planning', 'the'], dtype='<U32')},\n",
       " 41: {'story': array(['could', 'might', 'didn', 'seeing', 'laughed', 'got', 'greatly',\n",
       "         'getting', 'would', 'having'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'dining', 'meals', 'meal',\n",
       "         'waiter', 'cafe', 'sushi'], dtype='<U32'),\n",
       "  'soc': array(['the', 'of', 'first', 'two', 'interaction', 'what', 'next',\n",
       "         'other', 'time', 'when'], dtype='<U32')},\n",
       " 14: {'story': array(['couldn', 'could', 'don', 'can', 'doesn', 'didn', 'shouldn',\n",
       "         'make', 'sorry', 'hear'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'lectures', 'teaching', 'classroom',\n",
       "         'presentation', 'hall', 'semester', 'professor', 'lecturer'],\n",
       "        dtype='<U32'),\n",
       "  'soc': array(['breakup', 'breaking', 'splitting', 'for', 'the', 'proposing',\n",
       "         'break', 'relationship', 'of', 'up'], dtype='<U32')},\n",
       " 24: {'story': array(['can', 'during', 'ago', 'cannot', 'got', 'extremely', 'towards',\n",
       "         'knows', 'sorry', 'would'], dtype='<U32'),\n",
       "  'loc': array(['lecture', 'class', 'lectures', 'classroom', 'assignment', 'hall',\n",
       "         'lesson', 'semester', 'professor', 'lecturer'], dtype='<U32'),\n",
       "  'soc': array(['ring', 'diamond', 'rings', 'proposed', 'diamonds', 'proposing',\n",
       "         'planning', 'proposes', 'the', 'upcoming'], dtype='<U32')},\n",
       " 31: {'story': array(['wouldn', 'could', 'don', 'can', 'make', 'somewhat', 'would',\n",
       "         'knows', 'want', 'looks'], dtype='<U32'),\n",
       "  'loc': array(['restaurant', 'food', 'menus', 'menu', 'restuarant', 'meals',\n",
       "         'meal', 'waiter', 'hotel', 'eating'], dtype='<U32'),\n",
       "  'soc': array(['deal', 'partnership', 'business', 'agreement', 'acquisition',\n",
       "         'the', 'merger', 'competitor', 'proposal', 'decision'],\n",
       "        dtype='<U32')}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
