{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Analysis\n",
    "### 10/20/20\n",
    "### Comparing entropy across the different priming groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import copy\n",
    "from random import randrange\n",
    "import random\n",
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.stats import entropy\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Setup\n",
    "### i. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/analysis/parsing_log_file/pickle/data_dict_pilots_b1_b2_b3_b4_b5.p'\n",
    "data_dict = pickle.load( open( filename, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. import and organize story files\n",
    "#### a. Import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_stories = '/Users/alexreblando/Documents/GitHub/ebs/fMRI experiment/1:8:2020_edited_stories'\n",
    "\n",
    "filenames_stories = glob.glob(directory_stories + '/*.xlsx')\n",
    "\n",
    "dfs_stories = []\n",
    "    \n",
    "for filename in filenames_stories:\n",
    "    dfs_stories.append(pd.read_excel(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. organize putative event boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mark the boundaries with a '1'\n",
    "#output: story_boundaries, where the first column has the location boundaries and the second column has the social\n",
    "#boundaries\n",
    "\n",
    "story_boundaries = dict()\n",
    "\n",
    "#read in the location and social event values from the story files \n",
    "for s in range(16):\n",
    "    this_story = int(dfs_stories[s]['story'].iloc[0])\n",
    "    keys2 = dfs_stories[s]['locationEvent'].values\n",
    "    keys3 = dfs_stories[s]['socialEvent'].values\n",
    "    story_boundaries[this_story] = keys2[:, np.newaxis]\n",
    "    story_boundaries[this_story] = np.concatenate((story_boundaries[this_story], keys3[:, np.newaxis]), axis = 1)\n",
    "    \n",
    "#mark the changes or boundaries from one event to another with a '1' and delete first two columns\n",
    "for key in story_boundaries:\n",
    "    location = story_boundaries[key][:,0]\n",
    "    social = story_boundaries[key][:,1]\n",
    "    for i in range(0, len(location)):  \n",
    "        if location[i] > location[i-1]:\n",
    "            location[i] = 7\n",
    "    for i in range(0, len(location)):  \n",
    "        if location[i] != 7:\n",
    "            location[i] = 0\n",
    "    for i in range(0, len(social)):\n",
    "        if social[i] > social[i - 1]:\n",
    "            social[i] = 7\n",
    "    for i in range(0, len(social)):\n",
    "        if social[i] != 7:\n",
    "            social[i] = 0\n",
    "    location[0] = 7\n",
    "    social[0] = 7\n",
    "    location2 = location > 1\n",
    "    social2 = social > 1\n",
    "    location2 = location2.astype(int)\n",
    "    social2 = social2.astype(int)\n",
    "    story_boundaries[key] = np.concatenate((story_boundaries[key], location2[:, np.newaxis]), axis = 1) \n",
    "    story_boundaries[key] = np.concatenate((story_boundaries[key], social2[:, np.newaxis]), axis = 1)\n",
    "    story_boundaries[key] = np.delete(story_boundaries[key] ,np.s_[0:2],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Exclusions\n",
    "#### a. exclude participants who only pressed '1' or '9' (not including first row 10/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys_to_del = []\n",
    "for key in data_dict:\n",
    "    kp = data_dict[key]['story_presses']\n",
    "    kp_i = kp.iloc[1:]\n",
    "    check_kp_i = len(set(kp_i))\n",
    "    if check_kp_i == 1:\n",
    "        print(key)\n",
    "        print(data_dict[key]['prolific_id'].iloc[0])\n",
    "        keys_to_del.append(key)\n",
    "        \n",
    "for i in range(0, len(keys_to_del)):\n",
    "    del data_dict[keys_to_del[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Exclude participants who don't answer short answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/73_1_online_exp_2020-06-10_14h37.05.252'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f2a6c4981089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/73_1_online_exp_2020-06-10_14h37.05.252'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b2/30/199_1_online_exp_2020-06-24_20h41.05.680'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b3/45/345_1_online_exp_2020-07-16_09h07.09.087'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/73_1_online_exp_2020-06-10_14h37.05.252'"
     ]
    }
   ],
   "source": [
    "del data_dict['/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/73_1_online_exp_2020-06-10_14h37.05.252']\n",
    "del data_dict['/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b2/30/199_1_online_exp_2020-06-24_20h41.05.680']\n",
    "del data_dict['/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b3/45/345_1_online_exp_2020-07-16_09h07.09.087']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Exclude participants who said said more than half the sentences were a boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/pilot2/8/PARTICIPANT_1_online_exp_2020-05-11_20h42.51.597\n",
      "5ce5b9844cd8130019acba32\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/pilot4/7/PARTICIPANT_1_online_exp_2020-05-27_21h05.22.618\n",
      "5d46c556fb0ce0001a04fd01\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_p/10/18_1_online_exp_2020-06-06_14h58.27.858\n",
      "571ceeb64ca277000953d1c1\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_p/10/21_1_online_exp_2020-06-06_15h20.03.392\n",
      "5eda9f7c460b766ded73bdb7\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_p/10/24_1_online_exp_2020-06-09_20h39.35.048\n",
      "5ea616a090d52b61615dfb3b\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/152_1_online_exp_2020-06-10_21h15.29.074\n",
      "5d331b4c9ce6590019010930\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/174_1_online_exp_2020-06-10_21h49.17.497\n",
      "5c12ce5fd3f0d1000108430a \n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/177_1_online_exp_2020-06-10_14h45.30.620\n",
      "5eaa51883bf0061e3b4c5ea9\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/45_1_online_exp_2020-06-10_21h04.34.685\n",
      "598c282ccceb0f0001b39660\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b1/33/54_1_online_exp_2020-06-10_21h04.39.965\n",
      "5def01c28def535b96fa6133\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b2/30/195_1_online_exp_2020-06-24_09h34.43.267\n",
      "55a43687fdf99b7da1908e0f\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b3/45/361_1_online_exp_2020-07-16_04h07.12.067\n",
      "5ed478fa7d20a043b7c3be83\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b3/45/378_1_online_exp_2020-07-16_09h08.18.318\n",
      "5ebee71b8565892a3d0b5053\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b3/45/423_1_online_exp_2020-07-16_09h16.01.241\n",
      "5b200bbd325d600001c041c0\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b4/29/481_1_online_exp_2020-07-29_11h19.40.738\n",
      "5c3d4b92486a390001e87dfe\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b4/29/498_1_online_exp_2020-07-29_19h27.55.516\n",
      "566feba6b937e400052d33b2\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b5/78/102_1_online_exp_2020-09-28_15h37.58.717\n",
      "5f6053d2be7a76000a54da0a\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b5/78/130_1_online_exp_2020-09-28_21h06.20.693\n",
      "5f6f98d7e8a28b532f5a99d9\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b5/78/1_1_online_exp_2020-09-28_07h31.38.797\n",
      "5c3341bc21f99c000175c5cc\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b5/78/372_1_online_exp_2020-09-29_12h02.31.088\n",
      "5f58fdc20fcd710ce090cdc1\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b5/78/380_1_online_exp_2020-09-29_12h16.03.130\n",
      "5f50d96e44327d254d59bce7\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b5/78/406_1_online_exp_2020-09-30_19h26.43.920\n",
      "5f4cee33fa974e05995a92bd\n",
      "/Users/alexreblando/Documents/GitHub/data_analysis_ebs/online_experiment/data/finalver_b5/78/43_1_online_exp_2020-09-28_14h31.57.716\n",
      "5be8353e33f40a0001031d7a\n"
     ]
    }
   ],
   "source": [
    "keys_to_del = []\n",
    "\n",
    "for key in data_dict:\n",
    "    kp = data_dict[key]['story_presses']\n",
    "    N_kp= np.sum(kp == 9)\n",
    "    this_story = data_dict[key]['story'].iloc[0]\n",
    "    half_story_len = np.rint(len(story_boundaries[this_story])/2)\n",
    "    if N_kp >= half_story_len:\n",
    "        print(key)\n",
    "        print(data_dict[key]['prolific_id'].iloc[0])\n",
    "        keys_to_del.append(key)\n",
    "        \n",
    "for i in range(0, len(keys_to_del)):\n",
    "    del data_dict[keys_to_del[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Pickle data_dict in order to get group counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( data_dict, open( 'check', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Separating the Priming Groups For Each Story\n",
    "## A. group stories by subject and priming type\n",
    "#### - output: loc_keys_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc_keys_r = dict()\n",
    "soc_keys_r = dict()\n",
    "none_keys_r = dict()\n",
    "all_keys_r = dict()\n",
    "\n",
    "for key in data_dict:\n",
    "    this_story = data_dict[key]['story'].iloc[0]\n",
    "    keys = data_dict[key]['story_presses']\n",
    "    p_type = data_dict[key]['p_type'].iloc[0]\n",
    "    #print out the filenames and prolific ids of people who did not press \"9\" once\n",
    "    occurrences = np.count_nonzero(keys == 9)\n",
    "    if occurrences == 0:\n",
    "        print(key)\n",
    "        print(data_dict[key]['prolific_id'].iloc[0])\n",
    "    #put all story keys in the same dictionary regardless of priming\n",
    "    if this_story in all_keys_r:\n",
    "        all_keys_r[this_story] = np.concatenate((all_keys_r[this_story],keys[:,np.newaxis]), axis=1)\n",
    "    else:\n",
    "        all_keys_r[this_story] = keys[:, np.newaxis]\n",
    "    #organize keys by priming type\n",
    "    #no prime\n",
    "    if p_type == 0:\n",
    "        if this_story in none_keys_r:\n",
    "            none_keys_r[this_story] = np.concatenate((none_keys_r[this_story],keys[:,np.newaxis]), axis=1)\n",
    "        else:\n",
    "            none_keys_r[this_story] = keys[:, np.newaxis]\n",
    "    #loc prime\n",
    "    elif p_type == 1:\n",
    "        if this_story in loc_keys_r:\n",
    "            loc_keys_r[this_story] = np.concatenate((loc_keys_r[this_story],keys[:,np.newaxis]), axis=1)\n",
    "        else:\n",
    "            loc_keys_r[this_story] = keys[:, np.newaxis]\n",
    "    #soc prime\n",
    "    elif p_type == 2:\n",
    "        if this_story in soc_keys_r:\n",
    "            soc_keys_r[this_story] = np.concatenate((soc_keys_r[this_story],keys[:,np.newaxis]), axis=1)\n",
    "        else:\n",
    "            soc_keys_r[this_story] = keys[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Finding the number of participants in each category for each story\n",
    "#### -output sums[story] = [total # of participants, no prime, loc primed, soc primed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sums = {}\n",
    "\n",
    "for key in all_keys_r:\n",
    "    this_sum = []\n",
    "    x, y = all_keys_r[key].shape\n",
    "    this_sum.append(y)\n",
    "    if key in none_keys_r:\n",
    "        x, y = none_keys_r[key].shape\n",
    "        this_sum.append(y)\n",
    "    else:\n",
    "        this_sum.append(0)\n",
    "    if key in loc_keys_r:\n",
    "        x, y = loc_keys_r[key].shape\n",
    "        this_sum.append(y)\n",
    "    else:\n",
    "        this_sum.append(0)\n",
    "    if key in soc_keys_r:\n",
    "        x, y = soc_keys_r[key].shape\n",
    "        this_sum.append(y)\n",
    "    else:\n",
    "        this_sum.append(0)\n",
    "    sums[key] = this_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Convert keypresses from '9's and '1's to '1's and '0's\n",
    "#### - output loc_keys\n",
    "#### - WARNING: you can only run this once!!!!!! if you run twice, you must start from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc_keys = dict()\n",
    "soc_keys = dict()\n",
    "none_keys = dict()\n",
    "all_keys = dict()\n",
    "\n",
    "#all keys\n",
    "for key in all_keys_r:\n",
    "    this_array = copy.deepcopy(all_keys_r[key])\n",
    "    this_array[this_array ==1] = 0\n",
    "    this_array[this_array == 9] = 1\n",
    "    all_keys[key] = this_array   \n",
    "    \n",
    "#none keys\n",
    "for key in none_keys_r:\n",
    "    this_array = copy.deepcopy(none_keys_r[key])\n",
    "    this_array[this_array ==1] = 0\n",
    "    this_array[this_array == 9] = 1\n",
    "    none_keys[key] = this_array   \n",
    "    \n",
    "#loc keys\n",
    "for key in loc_keys_r:\n",
    "    this_array = copy.deepcopy(loc_keys_r[key])\n",
    "    this_array[this_array ==1] = 0\n",
    "    this_array[this_array == 9] = 1\n",
    "    loc_keys[key] = this_array   \n",
    "    \n",
    "#soc keys\n",
    "for key in soc_keys_r:\n",
    "    this_array = copy.deepcopy(soc_keys_r[key])\n",
    "    this_array[this_array ==1] = 0\n",
    "    this_array[this_array == 9] = 1\n",
    "    soc_keys[key] = this_array   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Remove first sentence (first row) from all keypresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_keys = [all_keys, none_keys, loc_keys, soc_keys]\n",
    "for i in range(0, len(list_keys)):\n",
    "    for key in list_keys[i]:\n",
    "        list_keys[i][key] = np.delete(list_keys[i][key],(0), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Remove first sentence (first row) from all a priori story boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in story_boundaries:\n",
    "    story_boundaries[key] = np.delete(story_boundaries[key],(0), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Entropy analysis\n",
    "## A. Entropy over all keypresses aggregated together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story_boundaries[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{11: 0.8836742372421339, 33: 0.9150999338801921, 44: 0.7745198684511658, 23: 0.9086273141664383, 12: 0.900396606879583, 21: 0.8039298728997144, 13: 0.7932182124737126, 42: 0.8039462551959011, 43: 0.8718754444498077, 32: 0.8823386330957198, 34: 0.7857891299991404, 22: 0.8345847232145776, 41: 0.8960950212677344, 14: 0.8505795285020237, 24: 0.9149991386009175, 31: 0.7309949373803546}\n"
     ]
    }
   ],
   "source": [
    "# Instantiating new dict and iterating through all_keys summing along row-wise\n",
    "sum_keys = {}\n",
    "\n",
    "for key in all_keys:\n",
    "    these_keys = all_keys[key]\n",
    "    sum_keys[key] = np.sum(these_keys, axis = 1)\n",
    "\n",
    "###    \n",
    "#ent[key] = entropy(sum_keys[key])/ln(N[key])    \n",
    "    \n",
    "# Plot each sum_keys array with loc boundaries\n",
    "ent = {}\n",
    "for key in sum_keys:\n",
    "    # entropy normalizes the vector\n",
    "    ent[key] = entropy(sum_keys[key])/np.log(len(story_boundaries[key]))\n",
    "    \n",
    "print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Entropy over different priming groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Instantiating new dicts and iterating through all_keys summing row-wise\n",
    "sum_keys_np = {}\n",
    "sum_keys_l = {}\n",
    "sum_keys_s = {}\n",
    "\n",
    "# Instantiating new dicts for ent\n",
    "ent_np = {}\n",
    "ent_l = {}\n",
    "ent_s = {}\n",
    "\n",
    "list_keys = [none_keys, loc_keys, soc_keys]\n",
    "list_sums = [sum_keys_np, sum_keys_l, sum_keys_s]\n",
    "list_ent = [ent_np, ent_l, ent_s]\n",
    "\n",
    "for i in range(0, len(list_keys)):\n",
    "    for key in list_keys[i]:\n",
    "        these_keys = list_keys[i][key]\n",
    "        list_sums[i][key] = np.sum(these_keys, axis = 1)\n",
    "\n",
    "\n",
    "for i in range(0, len(list_keys)):\n",
    "    print(i)\n",
    "    for key in list_keys[i]:\n",
    "        list_ent[i][key] = entropy(list_sums[i][key])/np.log(len(story_boundaries[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Averaging across all stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg entropy for np primed participants is  0.7320168839104153\n",
      "The avg entropy for loc primed participants is  0.7840862143399141\n",
      "The avg entropy for soc primed participants is  0.7576702921395695\n"
     ]
    }
   ],
   "source": [
    "story_list = [11,12,13,14,21,22,23,24,31,32,33,34,41,42,43,44]\n",
    "\n",
    "# Initiating array for storing value for each story\n",
    "np_vals = np.zeros((16))\n",
    "l_vals = np.zeros((16))\n",
    "s_vals = np.zeros((16))\n",
    "count = 0\n",
    "\n",
    "for j in story_list:\n",
    "    np_vals[count] = ent_np[j]\n",
    "    l_vals[count] = ent_l[j]\n",
    "    s_vals[count] = ent_s[j]\n",
    "    count += 1\n",
    "    \n",
    "avg_np = np.mean(np_vals)\n",
    "avg_l = np.mean(l_vals)\n",
    "avg_s = np.mean(s_vals)\n",
    "print(\"The avg entropy for np primed participants is \", avg_np)\n",
    "print(\"The avg entropy for loc primed participants is \", avg_l)\n",
    "print(\"The avg entropy for soc primed participants is \", avg_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
